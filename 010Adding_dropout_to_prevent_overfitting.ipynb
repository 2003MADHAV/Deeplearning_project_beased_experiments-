{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003MADHAV/Deeplearning_project_beased_experiments-/blob/main/010Adding_dropout_to_prevent_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5lRD_77zGB"
      },
      "source": [
        "#### Adding dropout to prevent overfitting\n",
        "Another popular method for regularization is dropout.\n",
        "\n",
        "A dropout forces a neural network to learn multiple independent representations by randomly removing connections between neurons in the learning phase.\n",
        "\n",
        "For example, when using a dropout of 0.5, the network has to see each example twice before the connection is learned.\n",
        "\n",
        "Therefore, a network with dropout can be seen as an ensemble of networks.\n",
        "\n",
        "Using the below code we will improve a model that clearly overfits the training data by adding dropouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RkaEeXb7zGE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghJ1FQ_57zGF"
      },
      "outputs": [],
      "source": [
        "# Dataset can be downloaded at https://archive.ics.uci.edu/ml/machine-learning-databases/00275/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_6HzEH07zGF",
        "outputId": "d3c6dd77-86dc-47b6-d04a-4aa71b961fbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17374</th>\n",
              "      <td>17375</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>11</td>\n",
              "      <td>108</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17375</th>\n",
              "      <td>17376</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>8</td>\n",
              "      <td>81</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17376</th>\n",
              "      <td>17377</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>7</td>\n",
              "      <td>83</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17377</th>\n",
              "      <td>17378</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>13</td>\n",
              "      <td>48</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17378</th>\n",
              "      <td>17379</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>12</td>\n",
              "      <td>37</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17379 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       instant      dteday  season  yr  mnth  hr  holiday  weekday  \\\n",
              "0            1  2011-01-01       1   0     1   0        0        6   \n",
              "1            2  2011-01-01       1   0     1   1        0        6   \n",
              "2            3  2011-01-01       1   0     1   2        0        6   \n",
              "3            4  2011-01-01       1   0     1   3        0        6   \n",
              "4            5  2011-01-01       1   0     1   4        0        6   \n",
              "...        ...         ...     ...  ..   ...  ..      ...      ...   \n",
              "17374    17375  2012-12-31       1   1    12  19        0        1   \n",
              "17375    17376  2012-12-31       1   1    12  20        0        1   \n",
              "17376    17377  2012-12-31       1   1    12  21        0        1   \n",
              "17377    17378  2012-12-31       1   1    12  22        0        1   \n",
              "17378    17379  2012-12-31       1   1    12  23        0        1   \n",
              "\n",
              "       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
              "0               0           1  0.24  0.2879  0.81     0.0000       3   \n",
              "1               0           1  0.22  0.2727  0.80     0.0000       8   \n",
              "2               0           1  0.22  0.2727  0.80     0.0000       5   \n",
              "3               0           1  0.24  0.2879  0.75     0.0000       3   \n",
              "4               0           1  0.24  0.2879  0.75     0.0000       0   \n",
              "...           ...         ...   ...     ...   ...        ...     ...   \n",
              "17374           1           2  0.26  0.2576  0.60     0.1642      11   \n",
              "17375           1           2  0.26  0.2576  0.60     0.1642       8   \n",
              "17376           1           1  0.26  0.2576  0.60     0.1642       7   \n",
              "17377           1           1  0.26  0.2727  0.56     0.1343      13   \n",
              "17378           1           1  0.26  0.2727  0.65     0.1343      12   \n",
              "\n",
              "       registered  cnt  \n",
              "0              13   16  \n",
              "1              32   40  \n",
              "2              27   32  \n",
              "3              10   13  \n",
              "4               1    1  \n",
              "...           ...  ...  \n",
              "17374         108  119  \n",
              "17375          81   89  \n",
              "17376          83   90  \n",
              "17377          48   61  \n",
              "17378          37   49  \n",
              "\n",
              "[17379 rows x 17 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('C:\\\\Users\\\\ifsrk\\\\Documents\\\\01 Deep Learning\\\\001 Handson\\\\hour.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtUKdErD7zGG",
        "outputId": "2dac4cf9-2a8c-4b55-dbce-44aa024f7c65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>...</th>\n",
              "      <th>hr_21</th>\n",
              "      <th>hr_22</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>weekday_0</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17374</th>\n",
              "      <td>17375</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17375</th>\n",
              "      <td>17376</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17376</th>\n",
              "      <td>17377</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17377</th>\n",
              "      <td>17378</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17378</th>\n",
              "      <td>17379</td>\n",
              "      <td>2012-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17379 rows × 68 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       instant      dteday  season  yr  mnth  hr  holiday  weekday  \\\n",
              "0            1  2011-01-01       1   0     1   0        0        6   \n",
              "1            2  2011-01-01       1   0     1   1        0        6   \n",
              "2            3  2011-01-01       1   0     1   2        0        6   \n",
              "3            4  2011-01-01       1   0     1   3        0        6   \n",
              "4            5  2011-01-01       1   0     1   4        0        6   \n",
              "...        ...         ...     ...  ..   ...  ..      ...      ...   \n",
              "17374    17375  2012-12-31       1   1    12  19        0        1   \n",
              "17375    17376  2012-12-31       1   1    12  20        0        1   \n",
              "17376    17377  2012-12-31       1   1    12  21        0        1   \n",
              "17377    17378  2012-12-31       1   1    12  22        0        1   \n",
              "17378    17379  2012-12-31       1   1    12  23        0        1   \n",
              "\n",
              "       workingday  weathersit  ...  hr_21  hr_22  hr_23  weekday_0  weekday_1  \\\n",
              "0               0           1  ...      0      0      0          0          0   \n",
              "1               0           1  ...      0      0      0          0          0   \n",
              "2               0           1  ...      0      0      0          0          0   \n",
              "3               0           1  ...      0      0      0          0          0   \n",
              "4               0           1  ...      0      0      0          0          0   \n",
              "...           ...         ...  ...    ...    ...    ...        ...        ...   \n",
              "17374           1           2  ...      0      0      0          0          1   \n",
              "17375           1           2  ...      0      0      0          0          1   \n",
              "17376           1           1  ...      1      0      0          0          1   \n",
              "17377           1           1  ...      0      1      0          0          1   \n",
              "17378           1           1  ...      0      0      1          0          1   \n",
              "\n",
              "       weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \n",
              "0              0          0          0          0          1  \n",
              "1              0          0          0          0          1  \n",
              "2              0          0          0          0          1  \n",
              "3              0          0          0          0          1  \n",
              "4              0          0          0          0          1  \n",
              "...          ...        ...        ...        ...        ...  \n",
              "17374          0          0          0          0          0  \n",
              "17375          0          0          0          0          0  \n",
              "17376          0          0          0          0          0  \n",
              "17377          0          0          0          0          0  \n",
              "17378          0          0          0          0          0  \n",
              "\n",
              "[17379 rows x 68 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature engineering\n",
        "ohe_features = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
        "for feature in ohe_features:\n",
        "    dummies = pd.get_dummies(data[feature], prefix=feature, drop_first=False)\n",
        "    data = pd.concat([data, dummies], axis=1)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RzNUN0I7zGG",
        "outputId": "ac4798d1-21f6-4c90-a8a2-0e2b4f554db0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>yr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>temp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>cnt</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>...</th>\n",
              "      <th>hr_21</th>\n",
              "      <th>hr_22</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>weekday_0</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17374</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>119</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17375</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>89</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17376</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17377</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>61</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17378</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17379 rows × 57 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       yr  holiday  temp   hum  windspeed  cnt  season_1  season_2  season_3  \\\n",
              "0       0        0  0.24  0.81     0.0000   16         1         0         0   \n",
              "1       0        0  0.22  0.80     0.0000   40         1         0         0   \n",
              "2       0        0  0.22  0.80     0.0000   32         1         0         0   \n",
              "3       0        0  0.24  0.75     0.0000   13         1         0         0   \n",
              "4       0        0  0.24  0.75     0.0000    1         1         0         0   \n",
              "...    ..      ...   ...   ...        ...  ...       ...       ...       ...   \n",
              "17374   1        0  0.26  0.60     0.1642  119         1         0         0   \n",
              "17375   1        0  0.26  0.60     0.1642   89         1         0         0   \n",
              "17376   1        0  0.26  0.60     0.1642   90         1         0         0   \n",
              "17377   1        0  0.26  0.56     0.1343   61         1         0         0   \n",
              "17378   1        0  0.26  0.65     0.1343   49         1         0         0   \n",
              "\n",
              "       season_4  ...  hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \\\n",
              "0             0  ...      0      0      0          0          0          0   \n",
              "1             0  ...      0      0      0          0          0          0   \n",
              "2             0  ...      0      0      0          0          0          0   \n",
              "3             0  ...      0      0      0          0          0          0   \n",
              "4             0  ...      0      0      0          0          0          0   \n",
              "...         ...  ...    ...    ...    ...        ...        ...        ...   \n",
              "17374         0  ...      0      0      0          0          1          0   \n",
              "17375         0  ...      0      0      0          0          1          0   \n",
              "17376         0  ...      1      0      0          0          1          0   \n",
              "17377         0  ...      0      1      0          0          1          0   \n",
              "17378         0  ...      0      0      1          0          1          0   \n",
              "\n",
              "       weekday_3  weekday_4  weekday_5  weekday_6  \n",
              "0              0          0          0          1  \n",
              "1              0          0          0          1  \n",
              "2              0          0          0          1  \n",
              "3              0          0          0          1  \n",
              "4              0          0          0          1  \n",
              "...          ...        ...        ...        ...  \n",
              "17374          0          0          0          0  \n",
              "17375          0          0          0          0  \n",
              "17376          0          0          0          0  \n",
              "17377          0          0          0          0  \n",
              "17378          0          0          0          0  \n",
              "\n",
              "[17379 rows x 57 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drop_features = ['instant', 'dteday', 'season', 'weathersit', 'weekday', 'atemp', 'mnth', 'workingday', 'hr', 'casual', 'registered']\n",
        "data = data.drop(drop_features, axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnFOky-x7zGH",
        "outputId": "68273487-0d24-4a6b-e6f2-c138119266b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'cnt': [189.46308763450142, 181.38759909186527],\n",
              " 'temp': [0.4969871684216586, 0.19255612124972407],\n",
              " 'hum': [0.6272288394038822, 0.1929298340629125],\n",
              " 'windspeed': [0.1900976063064631, 0.12234022857279413]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_features = ['cnt', 'temp', 'hum', 'windspeed']\n",
        "scaled_features = {}\n",
        "for feature in norm_features:\n",
        "    mean, std = data[feature].mean(), data[feature].std()\n",
        "    scaled_features[feature] = [mean, std]\n",
        "    data.loc[:, feature] = (data[feature] - mean)/std\n",
        "\n",
        "scaled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YvI9YiV7zGH"
      },
      "outputs": [],
      "source": [
        "# Save the final month for testing\n",
        "test_data = data[-31*24:]\n",
        "data = data[:-31*24]\n",
        "\n",
        "# Extract the target field\n",
        "target_fields = ['cnt']\n",
        "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
        "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]\n",
        "\n",
        "# Create a validation set (based on the last )\n",
        "X_train, y_train = features[:-30*24], targets[:-30*24]\n",
        "X_val, y_val = features[-30*24:], targets[-30*24:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOEtvy8X7zGI"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(250, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(150, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='mse', optimizer='sgd', metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM_V0iSP7zGI",
        "outputId": "86a0bef5-9414-4719-a020-51ba6b71b045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 250)               14250     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 150)               37650     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                7550      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 25)                1275      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 26        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,751\n",
            "Trainable params: 60,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12mCqxr_7zGI",
        "outputId": "92cf18a5-d45f-48b2-dad8-d4720ffe44dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot in d:\\users\\ifsrk\\anaconda3\\lib\\site-packages (1.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in d:\\users\\ifsrk\\anaconda3\\lib\\site-packages (from pydot) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAYUjtrf7zGI",
        "outputId": "e08e6d24-eddf-4b65-96f9-c6a6e2961175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'create'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9400/14350003.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dot\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"svg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Save the visualization as a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create'"
          ]
        }
      ],
      "source": [
        "# Visualize network architecture\n",
        "\n",
        "import pydot\n",
        "import pydotplus\n",
        "import graphviz\n",
        "from IPython.display import SVG\n",
        "#from tensorflow.keras.utils.vis_utils import model_to_dot\n",
        "#from tensorflow.keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True).create(prog=\"dot\", format=\"svg\"))\n",
        "\n",
        "# Save the visualization as a file\n",
        "plot_model(model, show_shapes=True, to_file=\"dropout_network_model.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ12jbLE7zGJ",
        "outputId": "ecb73665-0378-47c9-9846-d3f78e0e1521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "16/16 [==============================] - 2s 34ms/step - loss: 0.9941 - mse: 0.9941 - val_loss: 0.9825 - val_mse: 0.9825\n",
            "Epoch 2/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.9726 - mse: 0.9726 - val_loss: 0.9735 - val_mse: 0.9735\n",
            "Epoch 3/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.9476 - mse: 0.9476 - val_loss: 0.9609 - val_mse: 0.9609\n",
            "Epoch 4/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.9185 - mse: 0.9185 - val_loss: 0.9473 - val_mse: 0.9473\n",
            "Epoch 5/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.8847 - mse: 0.8847 - val_loss: 0.9316 - val_mse: 0.9316\n",
            "Epoch 6/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.8457 - mse: 0.8457 - val_loss: 0.9169 - val_mse: 0.9169\n",
            "Epoch 7/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.8027 - mse: 0.8027 - val_loss: 0.9031 - val_mse: 0.9031\n",
            "Epoch 8/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.7590 - mse: 0.7590 - val_loss: 0.8878 - val_mse: 0.8878\n",
            "Epoch 9/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.7182 - mse: 0.7182 - val_loss: 0.8724 - val_mse: 0.8724\n",
            "Epoch 10/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6833 - mse: 0.6833 - val_loss: 0.8579 - val_mse: 0.8579\n",
            "Epoch 11/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6546 - mse: 0.6546 - val_loss: 0.8468 - val_mse: 0.8468\n",
            "Epoch 12/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6305 - mse: 0.6305 - val_loss: 0.8257 - val_mse: 0.8257\n",
            "Epoch 13/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6091 - mse: 0.6091 - val_loss: 0.7952 - val_mse: 0.7952\n",
            "Epoch 14/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5886 - mse: 0.5886 - val_loss: 0.7578 - val_mse: 0.7578\n",
            "Epoch 15/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5690 - mse: 0.5690 - val_loss: 0.7357 - val_mse: 0.7357\n",
            "Epoch 16/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5489 - mse: 0.5489 - val_loss: 0.7138 - val_mse: 0.7138\n",
            "Epoch 17/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5289 - mse: 0.5289 - val_loss: 0.6762 - val_mse: 0.6762\n",
            "Epoch 18/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.5085 - mse: 0.5085 - val_loss: 0.6436 - val_mse: 0.6436\n",
            "Epoch 19/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.6310 - val_mse: 0.6310\n",
            "Epoch 20/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4674 - mse: 0.4674 - val_loss: 0.5924 - val_mse: 0.5924\n",
            "Epoch 21/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4457 - mse: 0.4457 - val_loss: 0.5735 - val_mse: 0.5735\n",
            "Epoch 22/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4247 - mse: 0.4247 - val_loss: 0.5437 - val_mse: 0.5437\n",
            "Epoch 23/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4036 - mse: 0.4036 - val_loss: 0.5229 - val_mse: 0.5229\n",
            "Epoch 24/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3833 - mse: 0.3833 - val_loss: 0.4999 - val_mse: 0.4999\n",
            "Epoch 25/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.3634 - mse: 0.3634 - val_loss: 0.4787 - val_mse: 0.4787\n",
            "Epoch 26/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3449 - mse: 0.3449 - val_loss: 0.4592 - val_mse: 0.4592\n",
            "Epoch 27/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.3276 - mse: 0.3276 - val_loss: 0.4411 - val_mse: 0.4411\n",
            "Epoch 28/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.3126 - mse: 0.3126 - val_loss: 0.4232 - val_mse: 0.4232\n",
            "Epoch 29/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2987 - mse: 0.2987 - val_loss: 0.4100 - val_mse: 0.4100\n",
            "Epoch 30/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2870 - mse: 0.2870 - val_loss: 0.3972 - val_mse: 0.3972\n",
            "Epoch 31/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2767 - mse: 0.2767 - val_loss: 0.3868 - val_mse: 0.3868\n",
            "Epoch 32/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3763 - val_mse: 0.3763\n",
            "Epoch 33/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.3686 - val_mse: 0.3686\n",
            "Epoch 34/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2547 - mse: 0.2547 - val_loss: 0.3637 - val_mse: 0.3637\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2495 - mse: 0.2495 - val_loss: 0.3561 - val_mse: 0.3561\n",
            "Epoch 36/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2442 - mse: 0.2442 - val_loss: 0.3522 - val_mse: 0.3522\n",
            "Epoch 37/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.3464 - val_mse: 0.3464\n",
            "Epoch 38/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2365 - mse: 0.2365 - val_loss: 0.3432 - val_mse: 0.3432\n",
            "Epoch 39/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.3396 - val_mse: 0.3396\n",
            "Epoch 40/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2286 - mse: 0.2286 - val_loss: 0.3391 - val_mse: 0.3391\n",
            "Epoch 41/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.2259 - mse: 0.2259 - val_loss: 0.3333 - val_mse: 0.3333\n",
            "Epoch 42/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2222 - mse: 0.2222 - val_loss: 0.3301 - val_mse: 0.3301\n",
            "Epoch 43/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2195 - mse: 0.2195 - val_loss: 0.3274 - val_mse: 0.3274\n",
            "Epoch 44/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2166 - mse: 0.2166 - val_loss: 0.3248 - val_mse: 0.3248\n",
            "Epoch 45/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2141 - mse: 0.2141 - val_loss: 0.3231 - val_mse: 0.3231\n",
            "Epoch 46/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2116 - mse: 0.2116 - val_loss: 0.3205 - val_mse: 0.3205\n",
            "Epoch 47/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2098 - mse: 0.2098 - val_loss: 0.3184 - val_mse: 0.3184\n",
            "Epoch 48/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2073 - mse: 0.2073 - val_loss: 0.3187 - val_mse: 0.3187\n",
            "Epoch 49/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2048 - mse: 0.2048 - val_loss: 0.3131 - val_mse: 0.3131\n",
            "Epoch 50/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2024 - mse: 0.2024 - val_loss: 0.3129 - val_mse: 0.3129\n",
            "Epoch 51/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2005 - mse: 0.2005 - val_loss: 0.3097 - val_mse: 0.3097\n",
            "Epoch 52/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1986 - mse: 0.1986 - val_loss: 0.3060 - val_mse: 0.3060\n",
            "Epoch 53/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1963 - mse: 0.1963 - val_loss: 0.3038 - val_mse: 0.3038\n",
            "Epoch 54/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1946 - mse: 0.1946 - val_loss: 0.3012 - val_mse: 0.3012\n",
            "Epoch 55/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1925 - mse: 0.1925 - val_loss: 0.3033 - val_mse: 0.3033\n",
            "Epoch 56/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1905 - mse: 0.1905 - val_loss: 0.3010 - val_mse: 0.3010\n",
            "Epoch 57/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.2958 - val_mse: 0.2958\n",
            "Epoch 58/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.2963 - val_mse: 0.2963\n",
            "Epoch 59/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1852 - mse: 0.1852 - val_loss: 0.2942 - val_mse: 0.2942\n",
            "Epoch 60/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1832 - mse: 0.1832 - val_loss: 0.2933 - val_mse: 0.2933\n",
            "Epoch 61/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1814 - mse: 0.1814 - val_loss: 0.2884 - val_mse: 0.2884\n",
            "Epoch 62/1000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1796 - mse: 0.1796 - val_loss: 0.2922 - val_mse: 0.2922\n",
            "Epoch 63/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1782 - mse: 0.1782 - val_loss: 0.2880 - val_mse: 0.2880\n",
            "Epoch 64/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1764 - mse: 0.1764 - val_loss: 0.2845 - val_mse: 0.2845\n",
            "Epoch 65/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1744 - mse: 0.1744 - val_loss: 0.2859 - val_mse: 0.2859\n",
            "Epoch 66/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1729 - mse: 0.1729 - val_loss: 0.2831 - val_mse: 0.2831\n",
            "Epoch 67/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1712 - mse: 0.1712 - val_loss: 0.2815 - val_mse: 0.2815\n",
            "Epoch 68/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1696 - mse: 0.1696 - val_loss: 0.2774 - val_mse: 0.2774\n",
            "Epoch 69/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1678 - mse: 0.1678 - val_loss: 0.2778 - val_mse: 0.2778\n",
            "Epoch 70/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1662 - mse: 0.1662 - val_loss: 0.2733 - val_mse: 0.2733\n",
            "Epoch 71/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1650 - mse: 0.1650 - val_loss: 0.2750 - val_mse: 0.2750\n",
            "Epoch 72/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1635 - mse: 0.1635 - val_loss: 0.2776 - val_mse: 0.2776\n",
            "Epoch 73/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1621 - mse: 0.1621 - val_loss: 0.2708 - val_mse: 0.2708\n",
            "Epoch 74/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1600 - mse: 0.1600 - val_loss: 0.2724 - val_mse: 0.2724\n",
            "Epoch 75/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1582 - mse: 0.1582 - val_loss: 0.2753 - val_mse: 0.2753\n",
            "Epoch 76/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1567 - mse: 0.1567 - val_loss: 0.2641 - val_mse: 0.2641\n",
            "Epoch 77/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.2611 - val_mse: 0.2611\n",
            "Epoch 78/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1534 - mse: 0.1534 - val_loss: 0.2581 - val_mse: 0.2581\n",
            "Epoch 79/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1519 - mse: 0.1519 - val_loss: 0.2596 - val_mse: 0.2596\n",
            "Epoch 80/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1501 - mse: 0.1501 - val_loss: 0.2644 - val_mse: 0.2644\n",
            "Epoch 81/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1493 - mse: 0.1493 - val_loss: 0.2539 - val_mse: 0.2539\n",
            "Epoch 82/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1480 - mse: 0.1480 - val_loss: 0.2541 - val_mse: 0.2541\n",
            "Epoch 83/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1462 - mse: 0.1462 - val_loss: 0.2514 - val_mse: 0.2514\n",
            "Epoch 84/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1441 - mse: 0.1441 - val_loss: 0.2524 - val_mse: 0.2524\n",
            "Epoch 85/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1424 - mse: 0.1424 - val_loss: 0.2512 - val_mse: 0.2512\n",
            "Epoch 86/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1407 - mse: 0.1407 - val_loss: 0.2479 - val_mse: 0.2479\n",
            "Epoch 87/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1395 - mse: 0.1395 - val_loss: 0.2486 - val_mse: 0.2486\n",
            "Epoch 88/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1379 - mse: 0.1379 - val_loss: 0.2488 - val_mse: 0.2488\n",
            "Epoch 89/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1366 - mse: 0.1366 - val_loss: 0.2392 - val_mse: 0.2392\n",
            "Epoch 90/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1353 - mse: 0.1353 - val_loss: 0.2405 - val_mse: 0.2405\n",
            "Epoch 91/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1338 - mse: 0.1338 - val_loss: 0.2395 - val_mse: 0.2395\n",
            "Epoch 92/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1320 - mse: 0.1320 - val_loss: 0.2392 - val_mse: 0.2392\n",
            "Epoch 93/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1307 - mse: 0.1307 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 94/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1295 - mse: 0.1295 - val_loss: 0.2372 - val_mse: 0.2372\n",
            "Epoch 95/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1276 - mse: 0.1276 - val_loss: 0.2374 - val_mse: 0.2374\n",
            "Epoch 96/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1263 - mse: 0.1263 - val_loss: 0.2385 - val_mse: 0.2385\n",
            "Epoch 97/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1249 - mse: 0.1249 - val_loss: 0.2332 - val_mse: 0.2332\n",
            "Epoch 98/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1235 - mse: 0.1235 - val_loss: 0.2283 - val_mse: 0.2283\n",
            "Epoch 99/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1218 - mse: 0.1218 - val_loss: 0.2324 - val_mse: 0.2324\n",
            "Epoch 100/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1205 - mse: 0.1205 - val_loss: 0.2303 - val_mse: 0.2303\n",
            "Epoch 101/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1194 - mse: 0.1194 - val_loss: 0.2252 - val_mse: 0.2252\n",
            "Epoch 102/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1181 - mse: 0.1181 - val_loss: 0.2282 - val_mse: 0.2282\n",
            "Epoch 103/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1163 - mse: 0.1163 - val_loss: 0.2278 - val_mse: 0.2278\n",
            "Epoch 104/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1151 - mse: 0.1151 - val_loss: 0.2252 - val_mse: 0.2252\n",
            "Epoch 105/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1138 - mse: 0.1138 - val_loss: 0.2142 - val_mse: 0.2142\n",
            "Epoch 106/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1124 - mse: 0.1124 - val_loss: 0.2246 - val_mse: 0.2246\n",
            "Epoch 107/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1115 - mse: 0.1115 - val_loss: 0.2138 - val_mse: 0.2138\n",
            "Epoch 108/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.2117 - val_mse: 0.2117\n",
            "Epoch 109/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1085 - mse: 0.1085 - val_loss: 0.2108 - val_mse: 0.2108\n",
            "Epoch 110/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1080 - mse: 0.1080 - val_loss: 0.2137 - val_mse: 0.2137\n",
            "Epoch 111/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1061 - mse: 0.1061 - val_loss: 0.2148 - val_mse: 0.2148\n",
            "Epoch 112/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.2084 - val_mse: 0.2084\n",
            "Epoch 113/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1036 - mse: 0.1036 - val_loss: 0.2104 - val_mse: 0.2104\n",
            "Epoch 114/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1022 - mse: 0.1022 - val_loss: 0.2051 - val_mse: 0.2051\n",
            "Epoch 115/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1020 - mse: 0.1020 - val_loss: 0.2079 - val_mse: 0.2079\n",
            "Epoch 116/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.2049 - val_mse: 0.2049\n",
            "Epoch 117/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.1971 - val_mse: 0.1971\n",
            "Epoch 118/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.2091 - val_mse: 0.2091\n",
            "Epoch 119/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0967 - mse: 0.0967 - val_loss: 0.2100 - val_mse: 0.2100\n",
            "Epoch 120/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.1944 - val_mse: 0.1944\n",
            "Epoch 121/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1952 - val_mse: 0.1952\n",
            "Epoch 122/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0939 - mse: 0.0939 - val_loss: 0.1970 - val_mse: 0.1970\n",
            "Epoch 123/1000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1962 - val_mse: 0.1962\n",
            "Epoch 124/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1886 - val_mse: 0.1886\n",
            "Epoch 125/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.1923 - val_mse: 0.1923\n",
            "Epoch 126/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1865 - val_mse: 0.1865\n",
            "Epoch 127/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1949 - val_mse: 0.1949\n",
            "Epoch 128/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.1953 - val_mse: 0.1953\n",
            "Epoch 129/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1901 - val_mse: 0.1901\n",
            "Epoch 130/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.1824 - val_mse: 0.1824\n",
            "Epoch 131/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1968 - val_mse: 0.1968\n",
            "Epoch 132/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0841 - mse: 0.0841 - val_loss: 0.1920 - val_mse: 0.1920\n",
            "Epoch 133/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0835 - mse: 0.0835 - val_loss: 0.1883 - val_mse: 0.1883\n",
            "Epoch 134/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.1835 - val_mse: 0.1835\n",
            "Epoch 135/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0814 - mse: 0.0814 - val_loss: 0.1822 - val_mse: 0.1822\n",
            "Epoch 136/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.1834 - val_mse: 0.1834\n",
            "Epoch 137/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0796 - mse: 0.0796 - val_loss: 0.1736 - val_mse: 0.1736\n",
            "Epoch 138/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0809 - mse: 0.0809 - val_loss: 0.1777 - val_mse: 0.1777\n",
            "Epoch 139/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0781 - mse: 0.0781 - val_loss: 0.1765 - val_mse: 0.1765\n",
            "Epoch 140/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0774 - mse: 0.0774 - val_loss: 0.1763 - val_mse: 0.1763\n",
            "Epoch 141/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1750 - val_mse: 0.1750\n",
            "Epoch 142/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0762 - mse: 0.0762 - val_loss: 0.1818 - val_mse: 0.1818\n",
            "Epoch 143/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0754 - mse: 0.0754 - val_loss: 0.1704 - val_mse: 0.1704\n",
            "Epoch 144/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0755 - mse: 0.0755 - val_loss: 0.1673 - val_mse: 0.1673\n",
            "Epoch 145/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0749 - mse: 0.0749 - val_loss: 0.1694 - val_mse: 0.1694\n",
            "Epoch 146/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0740 - mse: 0.0740 - val_loss: 0.1690 - val_mse: 0.1690\n",
            "Epoch 147/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0735 - mse: 0.0735 - val_loss: 0.1771 - val_mse: 0.1771\n",
            "Epoch 148/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0754 - mse: 0.0754 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "Epoch 149/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0748 - mse: 0.0748 - val_loss: 0.1773 - val_mse: 0.1773\n",
            "Epoch 150/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0729 - mse: 0.0729 - val_loss: 0.1691 - val_mse: 0.1691\n",
            "Epoch 151/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0707 - mse: 0.0707 - val_loss: 0.1791 - val_mse: 0.1791\n",
            "Epoch 152/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0717 - mse: 0.0717 - val_loss: 0.1799 - val_mse: 0.1799\n",
            "Epoch 153/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0723 - mse: 0.0723 - val_loss: 0.1703 - val_mse: 0.1703\n",
            "Epoch 154/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0696 - mse: 0.0696 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "Epoch 155/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 0.1786 - val_mse: 0.1786\n",
            "Epoch 156/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0690 - mse: 0.0690 - val_loss: 0.1620 - val_mse: 0.1620\n",
            "Epoch 157/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 0.1596 - val_mse: 0.1596\n",
            "Epoch 158/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0684 - mse: 0.0684 - val_loss: 0.1622 - val_mse: 0.1622\n",
            "Epoch 159/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0684 - mse: 0.0684 - val_loss: 0.1703 - val_mse: 0.1703\n",
            "Epoch 160/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0668 - mse: 0.0668 - val_loss: 0.1676 - val_mse: 0.1676\n",
            "Epoch 161/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0659 - mse: 0.0659 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "Epoch 162/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0668 - mse: 0.0668 - val_loss: 0.1686 - val_mse: 0.1686\n",
            "Epoch 163/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0666 - mse: 0.0666 - val_loss: 0.1806 - val_mse: 0.1806\n",
            "Epoch 164/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 165/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0679 - mse: 0.0679 - val_loss: 0.1542 - val_mse: 0.1542\n",
            "Epoch 166/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0684 - mse: 0.0684 - val_loss: 0.1559 - val_mse: 0.1559\n",
            "Epoch 167/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0649 - mse: 0.0649 - val_loss: 0.1564 - val_mse: 0.1564\n",
            "Epoch 168/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0641 - mse: 0.0641 - val_loss: 0.1622 - val_mse: 0.1622\n",
            "Epoch 169/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0638 - mse: 0.0638 - val_loss: 0.1695 - val_mse: 0.1695\n",
            "Epoch 170/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0630 - mse: 0.0630 - val_loss: 0.1744 - val_mse: 0.1744\n",
            "Epoch 171/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0660 - mse: 0.0660 - val_loss: 0.1772 - val_mse: 0.1772\n",
            "Epoch 172/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0651 - mse: 0.0651 - val_loss: 0.1527 - val_mse: 0.1527\n",
            "Epoch 173/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0616 - mse: 0.0616 - val_loss: 0.1523 - val_mse: 0.1523\n",
            "Epoch 174/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0629 - mse: 0.0629 - val_loss: 0.1504 - val_mse: 0.1504\n",
            "Epoch 175/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0613 - mse: 0.0613 - val_loss: 0.1593 - val_mse: 0.1593\n",
            "Epoch 176/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0605 - mse: 0.0605 - val_loss: 0.1566 - val_mse: 0.1566\n",
            "Epoch 177/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0607 - mse: 0.0607 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "Epoch 178/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0632 - mse: 0.0632 - val_loss: 0.1695 - val_mse: 0.1695\n",
            "Epoch 179/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0602 - mse: 0.0602 - val_loss: 0.1539 - val_mse: 0.1539\n",
            "Epoch 180/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0600 - mse: 0.0600 - val_loss: 0.1563 - val_mse: 0.1563\n",
            "Epoch 181/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0597 - mse: 0.0597 - val_loss: 0.1666 - val_mse: 0.1666\n",
            "Epoch 182/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0594 - mse: 0.0594 - val_loss: 0.1559 - val_mse: 0.1559\n",
            "Epoch 183/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0585 - mse: 0.0585 - val_loss: 0.1516 - val_mse: 0.1516\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 184/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0596 - mse: 0.0596 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 185/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0597 - mse: 0.0597 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 186/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0597 - mse: 0.0597 - val_loss: 0.1811 - val_mse: 0.1811\n",
            "Epoch 187/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0612 - mse: 0.0612 - val_loss: 0.1541 - val_mse: 0.1541\n",
            "Epoch 188/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0589 - mse: 0.0589 - val_loss: 0.1833 - val_mse: 0.1833\n",
            "Epoch 189/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0614 - mse: 0.0614 - val_loss: 0.1614 - val_mse: 0.1614\n",
            "Epoch 190/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0586 - mse: 0.0586 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 191/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0568 - mse: 0.0568 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 192/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0578 - mse: 0.0578 - val_loss: 0.1573 - val_mse: 0.1573\n",
            "Epoch 193/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0565 - mse: 0.0565 - val_loss: 0.1560 - val_mse: 0.1560\n",
            "Epoch 194/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0560 - mse: 0.0560 - val_loss: 0.1548 - val_mse: 0.1548\n",
            "Epoch 195/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0560 - mse: 0.0560 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 196/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0563 - mse: 0.0563 - val_loss: 0.1579 - val_mse: 0.1579\n",
            "Epoch 197/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0576 - mse: 0.0576 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "Epoch 198/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0563 - mse: 0.0563 - val_loss: 0.1735 - val_mse: 0.1735\n",
            "Epoch 199/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "Epoch 200/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0556 - mse: 0.0556 - val_loss: 0.1513 - val_mse: 0.1513\n",
            "Epoch 201/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0546 - mse: 0.0546 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "Epoch 202/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0545 - mse: 0.0545 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 203/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0541 - mse: 0.0541 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 204/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0546 - mse: 0.0546 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "Epoch 205/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.1622 - val_mse: 0.1622\n",
            "Epoch 206/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0547 - mse: 0.0547 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "Epoch 207/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0548 - mse: 0.0548 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 208/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0603 - mse: 0.0603 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 209/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0575 - mse: 0.0575 - val_loss: 0.1420 - val_mse: 0.1420\n",
            "Epoch 210/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0551 - mse: 0.0551 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 211/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.1754 - val_mse: 0.1754\n",
            "Epoch 212/1000\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0568 - mse: 0.0568 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 213/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0533 - mse: 0.0533 - val_loss: 0.1487 - val_mse: 0.1487\n",
            "Epoch 214/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0534 - mse: 0.0534 - val_loss: 0.1536 - val_mse: 0.1536\n",
            "Epoch 215/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0535 - mse: 0.0535 - val_loss: 0.1544 - val_mse: 0.1544\n",
            "Epoch 216/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0593 - mse: 0.0593 - val_loss: 0.1482 - val_mse: 0.1482\n",
            "Epoch 217/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0518 - mse: 0.0518 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "Epoch 218/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0518 - mse: 0.0518 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 219/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.1496 - val_mse: 0.1496\n",
            "Epoch 220/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0516 - mse: 0.0516 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 221/1000\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0536 - mse: 0.0536 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 222/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.1434 - val_mse: 0.1434\n",
            "Epoch 223/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0583 - mse: 0.0583 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 224/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0514 - mse: 0.0514 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 225/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.1435 - val_mse: 0.1435\n",
            "Epoch 226/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 227/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0523 - mse: 0.0523 - val_loss: 0.1565 - val_mse: 0.1565\n",
            "Epoch 228/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "Epoch 229/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0559 - mse: 0.0559 - val_loss: 0.1447 - val_mse: 0.1447\n",
            "Epoch 230/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 231/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0546 - mse: 0.0546 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 232/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0546 - mse: 0.0546 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 233/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.1422 - val_mse: 0.1422\n",
            "Epoch 234/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 235/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 236/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0510 - mse: 0.0510 - val_loss: 0.1592 - val_mse: 0.1592\n",
            "Epoch 237/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.1404 - val_mse: 0.1404\n",
            "Epoch 238/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 239/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0492 - mse: 0.0492 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "Epoch 240/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0515 - mse: 0.0515 - val_loss: 0.1381 - val_mse: 0.1381\n",
            "Epoch 241/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0535 - mse: 0.0535 - val_loss: 0.1387 - val_mse: 0.1387\n",
            "Epoch 242/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0533 - mse: 0.0533 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 243/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.1401 - val_mse: 0.1401\n",
            "Epoch 244/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 0.1469 - val_mse: 0.1469\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 245/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.1401 - val_mse: 0.1401\n",
            "Epoch 246/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0486 - mse: 0.0486 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 247/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 248/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0489 - mse: 0.0489 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 249/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0519 - mse: 0.0519 - val_loss: 0.1378 - val_mse: 0.1378\n",
            "Epoch 250/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0489 - mse: 0.0489 - val_loss: 0.1433 - val_mse: 0.1433\n",
            "Epoch 251/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0476 - mse: 0.0476 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "Epoch 252/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0480 - mse: 0.0480 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 253/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 254/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0480 - mse: 0.0480 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "Epoch 255/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 256/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0468 - mse: 0.0468 - val_loss: 0.1441 - val_mse: 0.1441\n",
            "Epoch 257/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0466 - mse: 0.0466 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 258/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0490 - mse: 0.0490 - val_loss: 0.1544 - val_mse: 0.1544\n",
            "Epoch 259/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0470 - mse: 0.0470 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 260/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.1542 - val_mse: 0.1542\n",
            "Epoch 261/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0525 - mse: 0.0525 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 262/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0467 - mse: 0.0467 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 263/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0461 - mse: 0.0461 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 264/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0462 - mse: 0.0462 - val_loss: 0.1484 - val_mse: 0.1484\n",
            "Epoch 265/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0467 - mse: 0.0467 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 266/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0469 - mse: 0.0469 - val_loss: 0.1400 - val_mse: 0.1400\n",
            "Epoch 267/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0467 - mse: 0.0467 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 268/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 269/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 270/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0481 - mse: 0.0481 - val_loss: 0.1588 - val_mse: 0.1588\n",
            "Epoch 271/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 272/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0493 - mse: 0.0493 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 273/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 274/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.1404 - val_mse: 0.1404\n",
            "Epoch 275/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0463 - mse: 0.0463 - val_loss: 0.1367 - val_mse: 0.1367\n",
            "Epoch 276/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 277/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0453 - mse: 0.0453 - val_loss: 0.1376 - val_mse: 0.1376\n",
            "Epoch 278/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.1383 - val_mse: 0.1383\n",
            "Epoch 279/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0468 - mse: 0.0468 - val_loss: 0.1366 - val_mse: 0.1366\n",
            "Epoch 280/1000\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.1374 - val_mse: 0.1374\n",
            "Epoch 281/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 282/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.1402 - val_mse: 0.1402\n",
            "Epoch 283/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "Epoch 284/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.1504 - val_mse: 0.1504\n",
            "Epoch 285/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0454 - mse: 0.0454 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 286/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.1397 - val_mse: 0.1397\n",
            "Epoch 287/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0465 - mse: 0.0465 - val_loss: 0.1366 - val_mse: 0.1366\n",
            "Epoch 288/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0461 - mse: 0.0461 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 289/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.1597 - val_mse: 0.1597\n",
            "Epoch 290/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 291/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 292/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0481 - mse: 0.0481 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 293/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 294/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 295/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 296/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 297/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.1444 - val_mse: 0.1444\n",
            "Epoch 298/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.1364 - val_mse: 0.1364\n",
            "Epoch 299/1000\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0464 - mse: 0.0464 - val_loss: 0.1374 - val_mse: 0.1374\n",
            "Epoch 300/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "Epoch 301/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.1397 - val_mse: 0.1397\n",
            "Epoch 302/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 303/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.1381 - val_mse: 0.1381\n",
            "Epoch 304/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 305/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.1381 - val_mse: 0.1381\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 306/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 307/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 308/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 309/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.1387 - val_mse: 0.1387\n",
            "Epoch 310/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0464 - mse: 0.0464 - val_loss: 0.1370 - val_mse: 0.1370\n",
            "Epoch 311/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.1376 - val_mse: 0.1376\n",
            "Epoch 312/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 313/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.1540 - val_mse: 0.1540\n",
            "Epoch 314/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 315/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 316/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.1377 - val_mse: 0.1377\n",
            "Epoch 317/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 318/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "Epoch 319/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 320/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.1386 - val_mse: 0.1386\n",
            "Epoch 321/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0465 - mse: 0.0465 - val_loss: 0.1405 - val_mse: 0.1405\n",
            "Epoch 322/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 323/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 324/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.1381 - val_mse: 0.1381\n",
            "Epoch 325/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.1570 - val_mse: 0.1570\n",
            "Epoch 326/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.1420 - val_mse: 0.1420\n",
            "Epoch 327/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.1377 - val_mse: 0.1377\n",
            "Epoch 328/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "Epoch 329/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.1474 - val_mse: 0.1474\n",
            "Epoch 330/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.1372 - val_mse: 0.1372\n",
            "Epoch 331/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.1369 - val_mse: 0.1369\n",
            "Epoch 332/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 333/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 334/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.1380 - val_mse: 0.1380\n",
            "Epoch 335/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0440 - mse: 0.0440 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 336/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.1428 - val_mse: 0.1428\n",
            "Epoch 337/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.1400 - val_mse: 0.1400\n",
            "Epoch 338/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.1440 - val_mse: 0.1440\n",
            "Epoch 339/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.1434 - val_mse: 0.1434\n",
            "Epoch 340/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 341/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 342/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.1527 - val_mse: 0.1527\n",
            "Epoch 343/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.1527 - val_mse: 0.1527\n",
            "Epoch 344/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 345/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.1512 - val_mse: 0.1512\n",
            "Epoch 346/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0456 - mse: 0.0456 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "Epoch 347/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 348/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.1563 - val_mse: 0.1563\n",
            "Epoch 349/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 350/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.1483 - val_mse: 0.1483\n",
            "Epoch 351/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.1531 - val_mse: 0.1531\n",
            "Epoch 352/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 353/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.1434 - val_mse: 0.1434\n",
            "Epoch 354/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.1429 - val_mse: 0.1429\n",
            "Epoch 355/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.1631 - val_mse: 0.1631\n",
            "Epoch 356/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "Epoch 357/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "Epoch 358/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.1669 - val_mse: 0.1669\n",
            "Epoch 359/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 360/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 361/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.1369 - val_mse: 0.1369\n",
            "Epoch 362/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.1404 - val_mse: 0.1404\n",
            "Epoch 363/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 364/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 365/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.1399 - val_mse: 0.1399\n",
            "Epoch 366/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.1367 - val_mse: 0.1367\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 367/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.1422 - val_mse: 0.1422\n",
            "Epoch 368/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.1374 - val_mse: 0.1374\n",
            "Epoch 369/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.1364 - val_mse: 0.1364\n",
            "Epoch 370/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.1387 - val_mse: 0.1387\n",
            "Epoch 371/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 372/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 373/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "Epoch 374/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.1384 - val_mse: 0.1384\n",
            "Epoch 375/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.1383 - val_mse: 0.1383\n",
            "Epoch 376/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.1365 - val_mse: 0.1365\n",
            "Epoch 377/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.1373 - val_mse: 0.1373\n",
            "Epoch 378/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.1441 - val_mse: 0.1441\n",
            "Epoch 379/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.1375 - val_mse: 0.1375\n",
            "Epoch 380/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.1369 - val_mse: 0.1369\n",
            "Epoch 381/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "Epoch 382/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "Epoch 383/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.1399 - val_mse: 0.1399\n",
            "Epoch 384/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.1378 - val_mse: 0.1378\n",
            "Epoch 385/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 386/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.1432 - val_mse: 0.1432\n",
            "Epoch 387/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 388/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 389/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.1528 - val_mse: 0.1528\n",
            "Epoch 390/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 391/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 392/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "Epoch 393/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 394/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 395/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 396/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "Epoch 397/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.1665 - val_mse: 0.1665\n",
            "Epoch 398/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0462 - mse: 0.0462 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 399/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.1626 - val_mse: 0.1626\n",
            "Epoch 400/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.1627 - val_mse: 0.1627\n",
            "Epoch 401/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 402/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 403/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 404/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.1407 - val_mse: 0.1407\n",
            "Epoch 405/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.1418 - val_mse: 0.1418\n",
            "Epoch 406/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 407/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 408/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.1402 - val_mse: 0.1402\n",
            "Epoch 409/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 410/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 411/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.1373 - val_mse: 0.1373\n",
            "Epoch 412/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 413/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "Epoch 414/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.1430 - val_mse: 0.1430\n",
            "Epoch 415/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 416/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 417/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 418/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 419/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "Epoch 420/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 421/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 422/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.1664 - val_mse: 0.1664\n",
            "Epoch 423/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 424/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 425/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.1490 - val_mse: 0.1490\n",
            "Epoch 426/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.1506 - val_mse: 0.1506\n",
            "Epoch 427/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.1497 - val_mse: 0.1497\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 428/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 429/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 430/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 431/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 432/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 433/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 434/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.1597 - val_mse: 0.1597\n",
            "Epoch 435/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "Epoch 436/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 437/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "Epoch 438/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 439/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "Epoch 440/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.1558 - val_mse: 0.1558\n",
            "Epoch 441/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.1440 - val_mse: 0.1440\n",
            "Epoch 442/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "Epoch 443/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 444/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1432 - val_mse: 0.1432\n",
            "Epoch 445/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1535 - val_mse: 0.1535\n",
            "Epoch 446/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 447/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.1796 - val_mse: 0.1796\n",
            "Epoch 448/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0470 - mse: 0.0470 - val_loss: 0.1648 - val_mse: 0.1648\n",
            "Epoch 449/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.1626 - val_mse: 0.1626\n",
            "Epoch 450/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 451/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 452/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 453/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "Epoch 454/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.1630 - val_mse: 0.1630\n",
            "Epoch 455/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.1627 - val_mse: 0.1627\n",
            "Epoch 456/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1397 - val_mse: 0.1397\n",
            "Epoch 457/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "Epoch 458/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.1435 - val_mse: 0.1435\n",
            "Epoch 459/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 460/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 461/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 462/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 463/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "Epoch 464/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 465/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 466/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 467/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.1564 - val_mse: 0.1564\n",
            "Epoch 468/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "Epoch 469/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "Epoch 470/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1429 - val_mse: 0.1429\n",
            "Epoch 471/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1523 - val_mse: 0.1523\n",
            "Epoch 472/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.1429 - val_mse: 0.1429\n",
            "Epoch 473/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 474/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 475/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1727 - val_mse: 0.1727\n",
            "Epoch 476/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "Epoch 477/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 478/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "Epoch 479/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.1466 - val_mse: 0.1466\n",
            "Epoch 480/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1545 - val_mse: 0.1545\n",
            "Epoch 481/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "Epoch 482/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.1541 - val_mse: 0.1541\n",
            "Epoch 483/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.1575 - val_mse: 0.1575\n",
            "Epoch 484/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "Epoch 485/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.1533 - val_mse: 0.1533\n",
            "Epoch 486/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1532 - val_mse: 0.1532\n",
            "Epoch 487/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.1580 - val_mse: 0.1580\n",
            "Epoch 488/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.1629 - val_mse: 0.1629\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 489/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.1503 - val_mse: 0.1503\n",
            "Epoch 490/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 491/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "Epoch 492/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 493/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.1447 - val_mse: 0.1447\n",
            "Epoch 494/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 495/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.1496 - val_mse: 0.1496\n",
            "Epoch 496/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.1576 - val_mse: 0.1576\n",
            "Epoch 497/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.1579 - val_mse: 0.1579\n",
            "Epoch 498/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.1598 - val_mse: 0.1598\n",
            "Epoch 499/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 500/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "Epoch 501/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "Epoch 502/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 503/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 504/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 505/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.1535 - val_mse: 0.1535\n",
            "Epoch 506/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.1644 - val_mse: 0.1644\n",
            "Epoch 507/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.1647 - val_mse: 0.1647\n",
            "Epoch 508/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.1595 - val_mse: 0.1595\n",
            "Epoch 509/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 510/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 511/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.1474 - val_mse: 0.1474\n",
            "Epoch 512/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 513/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 514/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.1665 - val_mse: 0.1665\n",
            "Epoch 515/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1570 - val_mse: 0.1570\n",
            "Epoch 516/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 517/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.1444 - val_mse: 0.1444\n",
            "Epoch 518/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 519/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "Epoch 520/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.1481 - val_mse: 0.1481\n",
            "Epoch 521/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.1498 - val_mse: 0.1498\n",
            "Epoch 522/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.1505 - val_mse: 0.1505\n",
            "Epoch 523/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 524/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.1497 - val_mse: 0.1497\n",
            "Epoch 525/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.1556 - val_mse: 0.1556\n",
            "Epoch 526/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1538 - val_mse: 0.1538\n",
            "Epoch 527/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.1540 - val_mse: 0.1540\n",
            "Epoch 528/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1608 - val_mse: 0.1608\n",
            "Epoch 529/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 530/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.1420 - val_mse: 0.1420\n",
            "Epoch 531/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.1503 - val_mse: 0.1503\n",
            "Epoch 532/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.1694 - val_mse: 0.1694\n",
            "Epoch 533/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.1663 - val_mse: 0.1663\n",
            "Epoch 534/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.1566 - val_mse: 0.1566\n",
            "Epoch 535/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 536/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 537/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 538/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 539/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.1589 - val_mse: 0.1589\n",
            "Epoch 540/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.1610 - val_mse: 0.1610\n",
            "Epoch 541/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.1605 - val_mse: 0.1605\n",
            "Epoch 542/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.1555 - val_mse: 0.1555\n",
            "Epoch 543/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.1538 - val_mse: 0.1538\n",
            "Epoch 544/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 545/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.1509 - val_mse: 0.1509\n",
            "Epoch 546/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "Epoch 547/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "Epoch 548/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1463 - val_mse: 0.1463\n",
            "Epoch 549/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.1543 - val_mse: 0.1543\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 550/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.1722 - val_mse: 0.1722\n",
            "Epoch 551/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.1506 - val_mse: 0.1506\n",
            "Epoch 552/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "Epoch 553/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 554/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 555/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.1405 - val_mse: 0.1405\n",
            "Epoch 556/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.1418 - val_mse: 0.1418\n",
            "Epoch 557/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 558/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 559/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 560/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1550 - val_mse: 0.1550\n",
            "Epoch 561/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1475 - val_mse: 0.1475\n",
            "Epoch 562/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 563/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 564/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.1417 - val_mse: 0.1417\n",
            "Epoch 565/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 566/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.1594 - val_mse: 0.1594\n",
            "Epoch 567/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "Epoch 568/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.1598 - val_mse: 0.1598\n",
            "Epoch 569/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 570/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 571/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 572/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 573/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 574/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 575/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 576/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 577/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.1482 - val_mse: 0.1482\n",
            "Epoch 578/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.1540 - val_mse: 0.1540\n",
            "Epoch 579/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.1507 - val_mse: 0.1507\n",
            "Epoch 580/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.1854 - val_mse: 0.1854\n",
            "Epoch 581/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.1662 - val_mse: 0.1662\n",
            "Epoch 582/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 583/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.1598 - val_mse: 0.1598\n",
            "Epoch 584/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1547 - val_mse: 0.1547\n",
            "Epoch 585/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.1690 - val_mse: 0.1690\n",
            "Epoch 586/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.1781 - val_mse: 0.1781\n",
            "Epoch 587/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.1553 - val_mse: 0.1553\n",
            "Epoch 588/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 589/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "Epoch 590/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.1606 - val_mse: 0.1606\n",
            "Epoch 591/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.1644 - val_mse: 0.1644\n",
            "Epoch 592/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.1737 - val_mse: 0.1737\n",
            "Epoch 593/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.1615 - val_mse: 0.1615\n",
            "Epoch 594/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.1496 - val_mse: 0.1496\n",
            "Epoch 595/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 596/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.1529 - val_mse: 0.1529\n",
            "Epoch 597/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 598/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 599/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "Epoch 600/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.1441 - val_mse: 0.1441\n",
            "Epoch 601/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 602/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 603/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 604/1000\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.1547 - val_mse: 0.1547\n",
            "Epoch 605/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1555 - val_mse: 0.1555\n",
            "Epoch 606/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 607/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 608/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.1463 - val_mse: 0.1463\n",
            "Epoch 609/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 610/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.1440 - val_mse: 0.1440\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 611/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 612/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 613/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.1423 - val_mse: 0.1423\n",
            "Epoch 614/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 615/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.1531 - val_mse: 0.1531\n",
            "Epoch 616/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "Epoch 617/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.1566 - val_mse: 0.1566\n",
            "Epoch 618/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 619/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.1428 - val_mse: 0.1428\n",
            "Epoch 620/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 621/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 622/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.1504 - val_mse: 0.1504\n",
            "Epoch 623/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.1671 - val_mse: 0.1671\n",
            "Epoch 624/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.1539 - val_mse: 0.1539\n",
            "Epoch 625/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1663 - val_mse: 0.1663\n",
            "Epoch 626/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1558 - val_mse: 0.1558\n",
            "Epoch 627/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "Epoch 628/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.1548 - val_mse: 0.1548\n",
            "Epoch 629/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 630/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "Epoch 631/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.1434 - val_mse: 0.1434\n",
            "Epoch 632/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "Epoch 633/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.1544 - val_mse: 0.1544\n",
            "Epoch 634/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1507 - val_mse: 0.1507\n",
            "Epoch 635/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 636/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "Epoch 637/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1542 - val_mse: 0.1542\n",
            "Epoch 638/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.1541 - val_mse: 0.1541\n",
            "Epoch 639/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "Epoch 640/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 641/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.1463 - val_mse: 0.1463\n",
            "Epoch 642/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 643/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1503 - val_mse: 0.1503\n",
            "Epoch 644/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 645/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.1819 - val_mse: 0.1819\n",
            "Epoch 646/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 647/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.1531 - val_mse: 0.1531\n",
            "Epoch 648/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1731 - val_mse: 0.1731\n",
            "Epoch 649/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "Epoch 650/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 651/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "Epoch 652/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1497 - val_mse: 0.1497\n",
            "Epoch 653/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 654/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1548 - val_mse: 0.1548\n",
            "Epoch 655/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1559 - val_mse: 0.1559\n",
            "Epoch 656/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 657/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.1704 - val_mse: 0.1704\n",
            "Epoch 658/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1609 - val_mse: 0.1609\n",
            "Epoch 659/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.1631 - val_mse: 0.1631\n",
            "Epoch 660/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 661/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.1531 - val_mse: 0.1531\n",
            "Epoch 662/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.1600 - val_mse: 0.1600\n",
            "Epoch 663/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.1679 - val_mse: 0.1679\n",
            "Epoch 664/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 665/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 666/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1539 - val_mse: 0.1539\n",
            "Epoch 667/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 668/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 669/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1447 - val_mse: 0.1447\n",
            "Epoch 670/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "Epoch 671/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.1510 - val_mse: 0.1510\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 672/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 673/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 674/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1615 - val_mse: 0.1615\n",
            "Epoch 675/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.1551 - val_mse: 0.1551\n",
            "Epoch 676/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.1498 - val_mse: 0.1498\n",
            "Epoch 677/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1555 - val_mse: 0.1555\n",
            "Epoch 678/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1580 - val_mse: 0.1580\n",
            "Epoch 679/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 680/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 681/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 682/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 683/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.1482 - val_mse: 0.1482\n",
            "Epoch 684/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 685/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "Epoch 686/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "Epoch 687/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.1447 - val_mse: 0.1447\n",
            "Epoch 688/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "Epoch 689/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1487 - val_mse: 0.1487\n",
            "Epoch 690/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 691/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.1571 - val_mse: 0.1571\n",
            "Epoch 692/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 693/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 694/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1550 - val_mse: 0.1550\n",
            "Epoch 695/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1511 - val_mse: 0.1511\n",
            "Epoch 696/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 697/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.1528 - val_mse: 0.1528\n",
            "Epoch 698/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "Epoch 699/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.1724 - val_mse: 0.1724\n",
            "Epoch 700/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 701/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1543 - val_mse: 0.1543\n",
            "Epoch 702/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "Epoch 703/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.1548 - val_mse: 0.1548\n",
            "Epoch 704/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.1540 - val_mse: 0.1540\n",
            "Epoch 705/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "Epoch 706/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 707/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1642 - val_mse: 0.1642\n",
            "Epoch 708/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.1662 - val_mse: 0.1662\n",
            "Epoch 709/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.1716 - val_mse: 0.1716\n",
            "Epoch 710/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1595 - val_mse: 0.1595\n",
            "Epoch 711/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1504 - val_mse: 0.1504\n",
            "Epoch 712/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1609 - val_mse: 0.1609\n",
            "Epoch 713/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1643 - val_mse: 0.1643\n",
            "Epoch 714/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.1704 - val_mse: 0.1704\n",
            "Epoch 715/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.1669 - val_mse: 0.1669\n",
            "Epoch 716/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.1586 - val_mse: 0.1586\n",
            "Epoch 717/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 718/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1559 - val_mse: 0.1559\n",
            "Epoch 719/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1576 - val_mse: 0.1576\n",
            "Epoch 720/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "Epoch 721/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "Epoch 722/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 723/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.1543 - val_mse: 0.1543\n",
            "Epoch 724/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 725/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 726/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.1533 - val_mse: 0.1533\n",
            "Epoch 727/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 728/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 729/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 730/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 731/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 732/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1693 - val_mse: 0.1693\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 733/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 734/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 735/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 736/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.1555 - val_mse: 0.1555\n",
            "Epoch 737/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 738/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "Epoch 739/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 740/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 741/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "Epoch 742/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1629 - val_mse: 0.1629\n",
            "Epoch 743/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.1700 - val_mse: 0.1700\n",
            "Epoch 744/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.1856 - val_mse: 0.1856\n",
            "Epoch 745/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "Epoch 746/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.1741 - val_mse: 0.1741\n",
            "Epoch 747/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1599 - val_mse: 0.1599\n",
            "Epoch 748/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 749/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.1700 - val_mse: 0.1700\n",
            "Epoch 750/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.1882 - val_mse: 0.1882\n",
            "Epoch 751/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.1688 - val_mse: 0.1688\n",
            "Epoch 752/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1582 - val_mse: 0.1582\n",
            "Epoch 753/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 754/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1558 - val_mse: 0.1558\n",
            "Epoch 755/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1640 - val_mse: 0.1640\n",
            "Epoch 756/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1610 - val_mse: 0.1610\n",
            "Epoch 757/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 758/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.1569 - val_mse: 0.1569\n",
            "Epoch 759/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.1498 - val_mse: 0.1498\n",
            "Epoch 760/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "Epoch 761/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 762/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "Epoch 763/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 764/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.1503 - val_mse: 0.1503\n",
            "Epoch 765/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 766/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "Epoch 767/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1498 - val_mse: 0.1498\n",
            "Epoch 768/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 769/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 770/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "Epoch 771/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1481 - val_mse: 0.1481\n",
            "Epoch 772/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1466 - val_mse: 0.1466\n",
            "Epoch 773/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.1532 - val_mse: 0.1532\n",
            "Epoch 774/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1578 - val_mse: 0.1578\n",
            "Epoch 775/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.1697 - val_mse: 0.1697\n",
            "Epoch 776/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.1636 - val_mse: 0.1636\n",
            "Epoch 777/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 778/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1639 - val_mse: 0.1639\n",
            "Epoch 779/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.1641 - val_mse: 0.1641\n",
            "Epoch 780/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 781/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "Epoch 782/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "Epoch 783/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1527 - val_mse: 0.1527\n",
            "Epoch 784/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1574 - val_mse: 0.1574\n",
            "Epoch 785/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "Epoch 786/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "Epoch 787/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.1636 - val_mse: 0.1636\n",
            "Epoch 788/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.1616 - val_mse: 0.1616\n",
            "Epoch 789/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1512 - val_mse: 0.1512\n",
            "Epoch 790/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "Epoch 791/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 792/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 793/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.1482 - val_mse: 0.1482\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 794/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "Epoch 795/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1505 - val_mse: 0.1505\n",
            "Epoch 796/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1641 - val_mse: 0.1641\n",
            "Epoch 797/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1662 - val_mse: 0.1662\n",
            "Epoch 798/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.1678 - val_mse: 0.1678\n",
            "Epoch 799/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1604 - val_mse: 0.1604\n",
            "Epoch 800/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.1603 - val_mse: 0.1603\n",
            "Epoch 801/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.1651 - val_mse: 0.1651\n",
            "Epoch 802/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.1570 - val_mse: 0.1570\n",
            "Epoch 803/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 804/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.1509 - val_mse: 0.1509\n",
            "Epoch 805/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1545 - val_mse: 0.1545\n",
            "Epoch 806/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1660 - val_mse: 0.1660\n",
            "Epoch 807/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1714 - val_mse: 0.1714\n",
            "Epoch 808/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 809/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "Epoch 810/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.1544 - val_mse: 0.1544\n",
            "Epoch 811/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1536 - val_mse: 0.1536\n",
            "Epoch 812/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "Epoch 813/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1762 - val_mse: 0.1762\n",
            "Epoch 814/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.1654 - val_mse: 0.1654\n",
            "Epoch 815/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.1490 - val_mse: 0.1490\n",
            "Epoch 816/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1513 - val_mse: 0.1513\n",
            "Epoch 817/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.1692 - val_mse: 0.1692\n",
            "Epoch 818/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.1680 - val_mse: 0.1680\n",
            "Epoch 819/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1669 - val_mse: 0.1669\n",
            "Epoch 820/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 821/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.1525 - val_mse: 0.1525\n",
            "Epoch 822/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.1541 - val_mse: 0.1541\n",
            "Epoch 823/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.1484 - val_mse: 0.1484\n",
            "Epoch 824/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 825/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 826/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 827/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "Epoch 828/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.1658 - val_mse: 0.1658\n",
            "Epoch 829/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1744 - val_mse: 0.1744\n",
            "Epoch 830/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1901 - val_mse: 0.1901\n",
            "Epoch 831/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1583 - val_mse: 0.1583\n",
            "Epoch 832/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1620 - val_mse: 0.1620\n",
            "Epoch 833/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1682 - val_mse: 0.1682\n",
            "Epoch 834/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.1714 - val_mse: 0.1714\n",
            "Epoch 835/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.1600 - val_mse: 0.1600\n",
            "Epoch 836/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1648 - val_mse: 0.1648\n",
            "Epoch 837/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.1572 - val_mse: 0.1572\n",
            "Epoch 838/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 839/1000\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 840/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1617 - val_mse: 0.1617\n",
            "Epoch 841/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.1579 - val_mse: 0.1579\n",
            "Epoch 842/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1580 - val_mse: 0.1580\n",
            "Epoch 843/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 844/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "Epoch 845/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "Epoch 846/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "Epoch 847/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.1544 - val_mse: 0.1544\n",
            "Epoch 848/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1649 - val_mse: 0.1649\n",
            "Epoch 849/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 850/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.1579 - val_mse: 0.1579\n",
            "Epoch 851/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.1570 - val_mse: 0.1570\n",
            "Epoch 852/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.1563 - val_mse: 0.1563\n",
            "Epoch 853/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1576 - val_mse: 0.1576\n",
            "Epoch 854/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.1773 - val_mse: 0.1773\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 855/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.1742 - val_mse: 0.1742\n",
            "Epoch 856/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 857/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1571 - val_mse: 0.1571\n",
            "Epoch 858/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1656 - val_mse: 0.1656\n",
            "Epoch 859/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "Epoch 860/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1709 - val_mse: 0.1709\n",
            "Epoch 861/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "Epoch 862/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 863/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1520 - val_mse: 0.1520\n",
            "Epoch 864/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.1578 - val_mse: 0.1578\n",
            "Epoch 865/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1542 - val_mse: 0.1542\n",
            "Epoch 866/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 867/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "Epoch 868/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "Epoch 869/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.1648 - val_mse: 0.1648\n",
            "Epoch 870/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.1570 - val_mse: 0.1570\n",
            "Epoch 871/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.1506 - val_mse: 0.1506\n",
            "Epoch 872/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.1529 - val_mse: 0.1529\n",
            "Epoch 873/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.1632 - val_mse: 0.1632\n",
            "Epoch 874/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.1672 - val_mse: 0.1672\n",
            "Epoch 875/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.1607 - val_mse: 0.1607\n",
            "Epoch 876/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1724 - val_mse: 0.1724\n",
            "Epoch 877/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.1760 - val_mse: 0.1760\n",
            "Epoch 878/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "Epoch 879/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.1551 - val_mse: 0.1551\n",
            "Epoch 880/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.1511 - val_mse: 0.1511\n",
            "Epoch 881/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 882/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1777 - val_mse: 0.1777\n",
            "Epoch 883/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 884/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.1511 - val_mse: 0.1511\n",
            "Epoch 885/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1637 - val_mse: 0.1637\n",
            "Epoch 886/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.1564 - val_mse: 0.1564\n",
            "Epoch 887/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.1633 - val_mse: 0.1633\n",
            "Epoch 888/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1730 - val_mse: 0.1730\n",
            "Epoch 889/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 890/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1556 - val_mse: 0.1556\n",
            "Epoch 891/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 892/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1553 - val_mse: 0.1553\n",
            "Epoch 893/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "Epoch 894/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 895/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.1554 - val_mse: 0.1554\n",
            "Epoch 896/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1566 - val_mse: 0.1566\n",
            "Epoch 897/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1620 - val_mse: 0.1620\n",
            "Epoch 898/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 899/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.1503 - val_mse: 0.1503\n",
            "Epoch 900/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 901/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "Epoch 902/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 903/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.1678 - val_mse: 0.1678\n",
            "Epoch 904/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.1726 - val_mse: 0.1726\n",
            "Epoch 905/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.1707 - val_mse: 0.1707\n",
            "Epoch 906/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "Epoch 907/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.1704 - val_mse: 0.1704\n",
            "Epoch 908/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1569 - val_mse: 0.1569\n",
            "Epoch 909/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1555 - val_mse: 0.1555\n",
            "Epoch 910/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1545 - val_mse: 0.1545\n",
            "Epoch 911/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 912/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1524 - val_mse: 0.1524\n",
            "Epoch 913/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1701 - val_mse: 0.1701\n",
            "Epoch 914/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.1714 - val_mse: 0.1714\n",
            "Epoch 915/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1662 - val_mse: 0.1662\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 916/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 917/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.1614 - val_mse: 0.1614\n",
            "Epoch 918/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.1907 - val_mse: 0.1907\n",
            "Epoch 919/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.1681 - val_mse: 0.1681\n",
            "Epoch 920/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.1654 - val_mse: 0.1654\n",
            "Epoch 921/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.1632 - val_mse: 0.1632\n",
            "Epoch 922/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.1632 - val_mse: 0.1632\n",
            "Epoch 923/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.1633 - val_mse: 0.1633\n",
            "Epoch 924/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1732 - val_mse: 0.1732\n",
            "Epoch 925/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1595 - val_mse: 0.1595\n",
            "Epoch 926/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1658 - val_mse: 0.1658\n",
            "Epoch 927/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 928/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1558 - val_mse: 0.1558\n",
            "Epoch 929/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1609 - val_mse: 0.1609\n",
            "Epoch 930/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1663 - val_mse: 0.1663\n",
            "Epoch 931/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 932/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1529 - val_mse: 0.1529\n",
            "Epoch 933/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.1608 - val_mse: 0.1608\n",
            "Epoch 934/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1599 - val_mse: 0.1599\n",
            "Epoch 935/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.1542 - val_mse: 0.1542\n",
            "Epoch 936/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.1538 - val_mse: 0.1538\n",
            "Epoch 937/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 938/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1537 - val_mse: 0.1537\n",
            "Epoch 939/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.1718 - val_mse: 0.1718\n",
            "Epoch 940/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 941/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "Epoch 942/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1520 - val_mse: 0.1520\n",
            "Epoch 943/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1605 - val_mse: 0.1605\n",
            "Epoch 944/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "Epoch 945/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "Epoch 946/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.1583 - val_mse: 0.1583\n",
            "Epoch 947/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 948/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 949/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1560 - val_mse: 0.1560\n",
            "Epoch 950/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.1556 - val_mse: 0.1556\n",
            "Epoch 951/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.1610 - val_mse: 0.1610\n",
            "Epoch 952/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "Epoch 953/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "Epoch 954/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 955/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 956/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 957/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1612 - val_mse: 0.1612\n",
            "Epoch 958/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1755 - val_mse: 0.1755\n",
            "Epoch 959/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1613 - val_mse: 0.1613\n",
            "Epoch 960/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1676 - val_mse: 0.1676\n",
            "Epoch 961/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 962/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1930 - val_mse: 0.1930\n",
            "Epoch 963/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.1718 - val_mse: 0.1718\n",
            "Epoch 964/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.1781 - val_mse: 0.1781\n",
            "Epoch 965/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1632 - val_mse: 0.1632\n",
            "Epoch 966/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "Epoch 967/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1660 - val_mse: 0.1660\n",
            "Epoch 968/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1692 - val_mse: 0.1692\n",
            "Epoch 969/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1622 - val_mse: 0.1622\n",
            "Epoch 970/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.1576 - val_mse: 0.1576\n",
            "Epoch 971/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.1621 - val_mse: 0.1621\n",
            "Epoch 972/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1780 - val_mse: 0.1780\n",
            "Epoch 973/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1658 - val_mse: 0.1658\n",
            "Epoch 974/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 975/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.1513 - val_mse: 0.1513\n",
            "Epoch 976/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1514 - val_mse: 0.1514\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 977/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.1529 - val_mse: 0.1529\n",
            "Epoch 978/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1607 - val_mse: 0.1607\n",
            "Epoch 979/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "Epoch 980/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1602 - val_mse: 0.1602\n",
            "Epoch 981/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.1627 - val_mse: 0.1627\n",
            "Epoch 982/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.1718 - val_mse: 0.1718\n",
            "Epoch 983/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1663 - val_mse: 0.1663\n",
            "Epoch 984/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.1762 - val_mse: 0.1762\n",
            "Epoch 985/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.1606 - val_mse: 0.1606\n",
            "Epoch 986/1000\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.1560 - val_mse: 0.1560\n",
            "Epoch 987/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.1556 - val_mse: 0.1556\n",
            "Epoch 988/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 989/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1511 - val_mse: 0.1511\n",
            "Epoch 990/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "Epoch 991/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.1623 - val_mse: 0.1623\n",
            "Epoch 992/1000\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.1858 - val_mse: 0.1858\n",
            "Epoch 993/1000\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 994/1000\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1615 - val_mse: 0.1615\n",
            "Epoch 995/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.1620 - val_mse: 0.1620\n",
            "Epoch 996/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 997/1000\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.1750 - val_mse: 0.1750\n",
            "Epoch 998/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.1665 - val_mse: 0.1665\n",
            "Epoch 999/1000\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "Epoch 1000/1000\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.1628 - val_mse: 0.1628\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 1000\n",
        "batch_size = 1024\n",
        "\n",
        "history = model.fit(X_train.values, y_train['cnt'],\n",
        "    validation_data=(X_val.values, y_val['cnt']),\n",
        "    batch_size=batch_size, epochs=n_epochs, verbose=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKMguMnU7zGJ",
        "outputId": "ee14caca-95a3-42a8-b49e-06b575619a24"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(np.arange(len(history.history['loss'])), history.history['loss'], label='training')\n",
        "plt.plot(np.arange(len(history.history['val_loss'])), history.history['val_loss'], label='validation')\n",
        "plt.title('Overfit on Bike Sharing dataset')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz0oJbkl7zGJ",
        "outputId": "2b963e1c-9f2b-40aa-d955-fbd658dc5d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum loss:  0.1364191323518753 \n",
            "After  368  epochs\n"
          ]
        }
      ],
      "source": [
        "print('Minimum loss: ', min(history.history['val_loss']),\n",
        " '\\nAfter ', np.argmin(history.history['val_loss']), ' epochs')\n",
        "\n",
        "# Minimum loss:  0.129907280207\n",
        "# After  980  epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2phTP5iR7zGJ"
      },
      "source": [
        "Dropout is a technique where randomly selected neurons are ignored during training.\n",
        "They are “dropped out” randomly.\n",
        "This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n",
        "\n",
        "As a neural network learns, neuron weights settle into their context within the network.\n",
        "Weights of neurons are tuned for specific features, providing some specialization. Neighboring neurons come to rely on this specialization, which, if taken too far, can result in a fragile model too specialized for the training data.\n",
        "This reliance on context for a neuron during training is referred to as complex co-adaptations.\n",
        "\n",
        "You can imagine that if neurons are randomly dropped out of the network during training, other neurons will have to step in and handle the representation required to make predictions for the missing neurons.\n",
        "This is believed to result in multiple independent internal representations being learned by the network.\n",
        "\n",
        "The effect is that the network becomes less sensitive to the specific weights of neurons.\n",
        "This, in turn, results in a network capable of better generalization and less likely to overfit the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JupaI-Ca7zGJ"
      },
      "outputs": [],
      "source": [
        "model_drop = Sequential()\n",
        "model_drop.add(Dense(250, input_dim=X_train.shape[1], activation='relu'))\n",
        "model_drop.add(Dropout(0.20))\n",
        "model_drop.add(Dense(150, activation='relu'))\n",
        "model_drop.add(Dropout(0.20))\n",
        "model_drop.add(Dense(50, activation='relu'))\n",
        "model_drop.add(Dropout(0.20))\n",
        "model_drop.add(Dense(25, activation='relu'))\n",
        "model_drop.add(Dropout(0.20))\n",
        "model_drop.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile model\n",
        "model_drop.compile(loss='mse', optimizer='sgd', metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JMQPV2tZ7zGJ",
        "outputId": "c8b6e80a-c418-4897-92d0-eae16d56bc23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 250)               14250     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 250)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 150)               37650     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 150)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 50)                7550      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 25)                1275      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 25)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 26        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,751\n",
            "Trainable params: 60,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_drop.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaOtFjur7zGK",
        "outputId": "37ee46a2-9348-45a0-f99b-0d52af0b18ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6914 - mse: 0.6914 - val_loss: 0.7712 - val_mse: 0.7712\n",
            "Epoch 2/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.7510 - val_mse: 0.7510\n",
            "Epoch 3/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6844 - mse: 0.6844 - val_loss: 0.7399 - val_mse: 0.7399\n",
            "Epoch 4/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6636 - mse: 0.6636 - val_loss: 0.7234 - val_mse: 0.7234\n",
            "Epoch 5/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6600 - mse: 0.6600 - val_loss: 0.7005 - val_mse: 0.7005\n",
            "Epoch 6/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6573 - mse: 0.6573 - val_loss: 0.6902 - val_mse: 0.6902\n",
            "Epoch 7/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.6365 - mse: 0.6365 - val_loss: 0.6698 - val_mse: 0.6698\n",
            "Epoch 8/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 0.6581 - val_mse: 0.6581\n",
            "Epoch 9/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.6305 - mse: 0.6305 - val_loss: 0.6472 - val_mse: 0.6472\n",
            "Epoch 10/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.6177 - mse: 0.6177 - val_loss: 0.6284 - val_mse: 0.6284\n",
            "Epoch 11/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.6054 - mse: 0.6054 - val_loss: 0.6138 - val_mse: 0.6138\n",
            "Epoch 12/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.5994 - mse: 0.5994 - val_loss: 0.6053 - val_mse: 0.6053\n",
            "Epoch 13/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.5937 - mse: 0.5937 - val_loss: 0.5918 - val_mse: 0.5918\n",
            "Epoch 14/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.5835 - mse: 0.5835 - val_loss: 0.5808 - val_mse: 0.5808\n",
            "Epoch 15/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.5740 - mse: 0.5740 - val_loss: 0.5686 - val_mse: 0.5686\n",
            "Epoch 16/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.5763 - mse: 0.5763 - val_loss: 0.5567 - val_mse: 0.5567\n",
            "Epoch 17/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.5528 - mse: 0.5528 - val_loss: 0.5453 - val_mse: 0.5453\n",
            "Epoch 18/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.5429 - mse: 0.5429 - val_loss: 0.5341 - val_mse: 0.5341\n",
            "Epoch 19/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.5387 - mse: 0.5387 - val_loss: 0.5242 - val_mse: 0.5242\n",
            "Epoch 20/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.5259 - mse: 0.5259 - val_loss: 0.5137 - val_mse: 0.5137\n",
            "Epoch 21/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.5248 - mse: 0.5248 - val_loss: 0.5054 - val_mse: 0.5054\n",
            "Epoch 22/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.5114 - mse: 0.5114 - val_loss: 0.4955 - val_mse: 0.4955\n",
            "Epoch 23/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.5074 - mse: 0.5074 - val_loss: 0.4871 - val_mse: 0.4871\n",
            "Epoch 24/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4905 - mse: 0.4905 - val_loss: 0.4767 - val_mse: 0.4767\n",
            "Epoch 25/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4897 - mse: 0.4897 - val_loss: 0.4679 - val_mse: 0.4679\n",
            "Epoch 26/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.4828 - mse: 0.4828 - val_loss: 0.4612 - val_mse: 0.4612\n",
            "Epoch 27/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.4768 - mse: 0.4768 - val_loss: 0.4534 - val_mse: 0.4534\n",
            "Epoch 28/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4725 - mse: 0.4725 - val_loss: 0.4481 - val_mse: 0.4481\n",
            "Epoch 29/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.4398 - val_mse: 0.4398\n",
            "Epoch 30/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4565 - mse: 0.4565 - val_loss: 0.4329 - val_mse: 0.4329\n",
            "Epoch 31/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4418 - mse: 0.4418 - val_loss: 0.4262 - val_mse: 0.4262\n",
            "Epoch 32/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4432 - mse: 0.4432 - val_loss: 0.4221 - val_mse: 0.4221\n",
            "Epoch 33/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4428 - mse: 0.4428 - val_loss: 0.4158 - val_mse: 0.4158\n",
            "Epoch 34/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4371 - mse: 0.4371 - val_loss: 0.4104 - val_mse: 0.4104\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4280 - mse: 0.4280 - val_loss: 0.4059 - val_mse: 0.4059\n",
            "Epoch 36/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4315 - mse: 0.4315 - val_loss: 0.4032 - val_mse: 0.4032\n",
            "Epoch 37/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4251 - mse: 0.4251 - val_loss: 0.3992 - val_mse: 0.3992\n",
            "Epoch 38/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4165 - mse: 0.4165 - val_loss: 0.3948 - val_mse: 0.3948\n",
            "Epoch 39/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4101 - mse: 0.4101 - val_loss: 0.3897 - val_mse: 0.3897\n",
            "Epoch 40/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4070 - mse: 0.4070 - val_loss: 0.3853 - val_mse: 0.3853\n",
            "Epoch 41/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4011 - mse: 0.4011 - val_loss: 0.3833 - val_mse: 0.3833\n",
            "Epoch 42/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4012 - mse: 0.4012 - val_loss: 0.3802 - val_mse: 0.3802\n",
            "Epoch 43/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3970 - mse: 0.3970 - val_loss: 0.3774 - val_mse: 0.3774\n",
            "Epoch 44/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3941 - mse: 0.3941 - val_loss: 0.3738 - val_mse: 0.3738\n",
            "Epoch 45/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3832 - mse: 0.3832 - val_loss: 0.3717 - val_mse: 0.3717\n",
            "Epoch 46/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3825 - mse: 0.3825 - val_loss: 0.3689 - val_mse: 0.3689\n",
            "Epoch 47/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.3839 - mse: 0.3839 - val_loss: 0.3660 - val_mse: 0.3660\n",
            "Epoch 48/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3818 - mse: 0.3818 - val_loss: 0.3638 - val_mse: 0.3638\n",
            "Epoch 49/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3774 - mse: 0.3774 - val_loss: 0.3604 - val_mse: 0.3604\n",
            "Epoch 50/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3709 - mse: 0.3709 - val_loss: 0.3602 - val_mse: 0.3602\n",
            "Epoch 51/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3671 - mse: 0.3671 - val_loss: 0.3578 - val_mse: 0.3578\n",
            "Epoch 52/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3660 - mse: 0.3660 - val_loss: 0.3550 - val_mse: 0.3550\n",
            "Epoch 53/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3631 - mse: 0.3631 - val_loss: 0.3540 - val_mse: 0.3540\n",
            "Epoch 54/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3589 - mse: 0.3589 - val_loss: 0.3520 - val_mse: 0.3520\n",
            "Epoch 55/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3603 - mse: 0.3603 - val_loss: 0.3492 - val_mse: 0.3492\n",
            "Epoch 56/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3584 - mse: 0.3584 - val_loss: 0.3482 - val_mse: 0.3482\n",
            "Epoch 57/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3538 - mse: 0.3538 - val_loss: 0.3467 - val_mse: 0.3467\n",
            "Epoch 58/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3501 - mse: 0.3501 - val_loss: 0.3461 - val_mse: 0.3461\n",
            "Epoch 59/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3367 - mse: 0.3367 - val_loss: 0.3434 - val_mse: 0.3434\n",
            "Epoch 60/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3432 - mse: 0.3432 - val_loss: 0.3426 - val_mse: 0.3426\n",
            "Epoch 61/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3486 - mse: 0.3486 - val_loss: 0.3410 - val_mse: 0.3410\n",
            "Epoch 62/1000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3398 - mse: 0.3398 - val_loss: 0.3398 - val_mse: 0.3398\n",
            "Epoch 63/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3418 - mse: 0.3418 - val_loss: 0.3392 - val_mse: 0.3392\n",
            "Epoch 64/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.3381 - mse: 0.3381 - val_loss: 0.3375 - val_mse: 0.3375\n",
            "Epoch 65/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.3344 - mse: 0.3344 - val_loss: 0.3356 - val_mse: 0.3356\n",
            "Epoch 66/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.3342 - mse: 0.3342 - val_loss: 0.3340 - val_mse: 0.3340\n",
            "Epoch 67/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3345 - mse: 0.3345 - val_loss: 0.3326 - val_mse: 0.3326\n",
            "Epoch 68/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.3264 - mse: 0.3264 - val_loss: 0.3309 - val_mse: 0.3309\n",
            "Epoch 69/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.3217 - mse: 0.3217 - val_loss: 0.3298 - val_mse: 0.3298\n",
            "Epoch 70/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.3226 - mse: 0.3226 - val_loss: 0.3276 - val_mse: 0.3276\n",
            "Epoch 71/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3190 - mse: 0.3190 - val_loss: 0.3262 - val_mse: 0.3262\n",
            "Epoch 72/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.3262 - mse: 0.3262 - val_loss: 0.3252 - val_mse: 0.3252\n",
            "Epoch 73/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3233 - mse: 0.3233 - val_loss: 0.3248 - val_mse: 0.3248\n",
            "Epoch 74/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3173 - mse: 0.3173 - val_loss: 0.3232 - val_mse: 0.3232\n",
            "Epoch 75/1000\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.3235 - mse: 0.3235 - val_loss: 0.3227 - val_mse: 0.3227\n",
            "Epoch 76/1000\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.3205 - mse: 0.3205 - val_loss: 0.3212 - val_mse: 0.3212\n",
            "Epoch 77/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.3129 - mse: 0.3129 - val_loss: 0.3198 - val_mse: 0.3198\n",
            "Epoch 78/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.3133 - mse: 0.3133 - val_loss: 0.3194 - val_mse: 0.3194\n",
            "Epoch 79/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.3116 - mse: 0.3116 - val_loss: 0.3174 - val_mse: 0.3174\n",
            "Epoch 80/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.3104 - mse: 0.3104 - val_loss: 0.3164 - val_mse: 0.3164\n",
            "Epoch 81/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3088 - mse: 0.3088 - val_loss: 0.3156 - val_mse: 0.3156\n",
            "Epoch 82/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3035 - mse: 0.3035 - val_loss: 0.3135 - val_mse: 0.3135\n",
            "Epoch 83/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3061 - mse: 0.3061 - val_loss: 0.3122 - val_mse: 0.3122\n",
            "Epoch 84/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3033 - mse: 0.3033 - val_loss: 0.3125 - val_mse: 0.3125\n",
            "Epoch 85/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.2970 - mse: 0.2970 - val_loss: 0.3102 - val_mse: 0.3102\n",
            "Epoch 86/1000\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.3005 - mse: 0.3005 - val_loss: 0.3099 - val_mse: 0.3099\n",
            "Epoch 87/1000\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.2967 - mse: 0.2967 - val_loss: 0.3087 - val_mse: 0.3087\n",
            "Epoch 88/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2953 - mse: 0.2953 - val_loss: 0.3074 - val_mse: 0.3074\n",
            "Epoch 89/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2929 - mse: 0.2929 - val_loss: 0.3070 - val_mse: 0.3070\n",
            "Epoch 90/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2906 - mse: 0.2906 - val_loss: 0.3049 - val_mse: 0.3049\n",
            "Epoch 91/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2848 - mse: 0.2848 - val_loss: 0.3039 - val_mse: 0.3039\n",
            "Epoch 92/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.2884 - mse: 0.2884 - val_loss: 0.3025 - val_mse: 0.3025\n",
            "Epoch 93/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2935 - mse: 0.2935 - val_loss: 0.3018 - val_mse: 0.3018\n",
            "Epoch 94/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2830 - mse: 0.2830 - val_loss: 0.3006 - val_mse: 0.3006\n",
            "Epoch 95/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2873 - mse: 0.2873 - val_loss: 0.2985 - val_mse: 0.2985\n",
            "Epoch 96/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2853 - mse: 0.2853 - val_loss: 0.2979 - val_mse: 0.2979\n",
            "Epoch 97/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2850 - mse: 0.2850 - val_loss: 0.2968 - val_mse: 0.2968\n",
            "Epoch 98/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2759 - mse: 0.2759 - val_loss: 0.2957 - val_mse: 0.2957\n",
            "Epoch 99/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2784 - mse: 0.2784 - val_loss: 0.2948 - val_mse: 0.2948\n",
            "Epoch 100/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2809 - mse: 0.2809 - val_loss: 0.2946 - val_mse: 0.2946\n",
            "Epoch 101/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.2754 - mse: 0.2754 - val_loss: 0.2938 - val_mse: 0.2938\n",
            "Epoch 102/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2768 - mse: 0.2768 - val_loss: 0.2931 - val_mse: 0.2931\n",
            "Epoch 103/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2764 - mse: 0.2764 - val_loss: 0.2919 - val_mse: 0.2919\n",
            "Epoch 104/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2756 - mse: 0.2756 - val_loss: 0.2911 - val_mse: 0.2911\n",
            "Epoch 105/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.2894 - val_mse: 0.2894\n",
            "Epoch 106/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.2883 - val_mse: 0.2883\n",
            "Epoch 107/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2707 - mse: 0.2707 - val_loss: 0.2878 - val_mse: 0.2878\n",
            "Epoch 108/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2864 - val_mse: 0.2864\n",
            "Epoch 109/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2661 - mse: 0.2661 - val_loss: 0.2846 - val_mse: 0.2846\n",
            "Epoch 110/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2683 - mse: 0.2683 - val_loss: 0.2837 - val_mse: 0.2837\n",
            "Epoch 111/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2647 - mse: 0.2647 - val_loss: 0.2831 - val_mse: 0.2831\n",
            "Epoch 112/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2654 - mse: 0.2654 - val_loss: 0.2819 - val_mse: 0.2819\n",
            "Epoch 113/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2655 - mse: 0.2655 - val_loss: 0.2807 - val_mse: 0.2807\n",
            "Epoch 114/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2583 - mse: 0.2583 - val_loss: 0.2801 - val_mse: 0.2801\n",
            "Epoch 115/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2585 - mse: 0.2585 - val_loss: 0.2790 - val_mse: 0.2790\n",
            "Epoch 116/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2575 - mse: 0.2575 - val_loss: 0.2789 - val_mse: 0.2789\n",
            "Epoch 117/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2570 - mse: 0.2570 - val_loss: 0.2778 - val_mse: 0.2778\n",
            "Epoch 118/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2634 - mse: 0.2634 - val_loss: 0.2772 - val_mse: 0.2772\n",
            "Epoch 119/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2538 - mse: 0.2538 - val_loss: 0.2754 - val_mse: 0.2754\n",
            "Epoch 120/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2560 - mse: 0.2560 - val_loss: 0.2759 - val_mse: 0.2759\n",
            "Epoch 121/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2742 - val_mse: 0.2742\n",
            "Epoch 122/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2556 - mse: 0.2556 - val_loss: 0.2737 - val_mse: 0.2737\n",
            "Epoch 123/1000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2575 - mse: 0.2575 - val_loss: 0.2728 - val_mse: 0.2728\n",
            "Epoch 124/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.2715 - val_mse: 0.2715\n",
            "Epoch 125/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2469 - mse: 0.2469 - val_loss: 0.2702 - val_mse: 0.2702\n",
            "Epoch 126/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2561 - mse: 0.2561 - val_loss: 0.2692 - val_mse: 0.2692\n",
            "Epoch 127/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2481 - mse: 0.2481 - val_loss: 0.2680 - val_mse: 0.2680\n",
            "Epoch 128/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2472 - mse: 0.2472 - val_loss: 0.2666 - val_mse: 0.2666\n",
            "Epoch 129/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2442 - mse: 0.2442 - val_loss: 0.2656 - val_mse: 0.2656\n",
            "Epoch 130/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2484 - mse: 0.2484 - val_loss: 0.2654 - val_mse: 0.2654\n",
            "Epoch 131/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2457 - mse: 0.2457 - val_loss: 0.2645 - val_mse: 0.2645\n",
            "Epoch 132/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2637 - val_mse: 0.2637\n",
            "Epoch 133/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2436 - mse: 0.2436 - val_loss: 0.2630 - val_mse: 0.2630\n",
            "Epoch 134/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2431 - mse: 0.2431 - val_loss: 0.2620 - val_mse: 0.2620\n",
            "Epoch 135/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2609 - val_mse: 0.2609\n",
            "Epoch 136/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2601 - val_mse: 0.2601\n",
            "Epoch 137/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2390 - mse: 0.2390 - val_loss: 0.2599 - val_mse: 0.2599\n",
            "Epoch 138/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2365 - mse: 0.2365 - val_loss: 0.2587 - val_mse: 0.2587\n",
            "Epoch 139/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2338 - mse: 0.2338 - val_loss: 0.2577 - val_mse: 0.2577\n",
            "Epoch 140/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2333 - mse: 0.2333 - val_loss: 0.2574 - val_mse: 0.2574\n",
            "Epoch 141/1000\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.2402 - mse: 0.2402 - val_loss: 0.2570 - val_mse: 0.2570\n",
            "Epoch 142/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2311 - mse: 0.2311 - val_loss: 0.2557 - val_mse: 0.2557\n",
            "Epoch 143/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2377 - mse: 0.2377 - val_loss: 0.2547 - val_mse: 0.2547\n",
            "Epoch 144/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.2317 - mse: 0.2317 - val_loss: 0.2539 - val_mse: 0.2539\n",
            "Epoch 145/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2317 - mse: 0.2317 - val_loss: 0.2529 - val_mse: 0.2529\n",
            "Epoch 146/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2269 - mse: 0.2269 - val_loss: 0.2521 - val_mse: 0.2521\n",
            "Epoch 147/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2269 - mse: 0.2269 - val_loss: 0.2518 - val_mse: 0.2518\n",
            "Epoch 148/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2272 - mse: 0.2272 - val_loss: 0.2512 - val_mse: 0.2512\n",
            "Epoch 149/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2310 - mse: 0.2310 - val_loss: 0.2505 - val_mse: 0.2505\n",
            "Epoch 150/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2302 - mse: 0.2302 - val_loss: 0.2490 - val_mse: 0.2490\n",
            "Epoch 151/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2271 - mse: 0.2271 - val_loss: 0.2480 - val_mse: 0.2480\n",
            "Epoch 152/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2282 - mse: 0.2282 - val_loss: 0.2469 - val_mse: 0.2469\n",
            "Epoch 153/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2255 - mse: 0.2255 - val_loss: 0.2458 - val_mse: 0.2458\n",
            "Epoch 154/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2229 - mse: 0.2229 - val_loss: 0.2453 - val_mse: 0.2453\n",
            "Epoch 155/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.2267 - mse: 0.2267 - val_loss: 0.2441 - val_mse: 0.2441\n",
            "Epoch 156/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2150 - mse: 0.2150 - val_loss: 0.2439 - val_mse: 0.2439\n",
            "Epoch 157/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2242 - mse: 0.2242 - val_loss: 0.2426 - val_mse: 0.2426\n",
            "Epoch 158/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2189 - mse: 0.2189 - val_loss: 0.2416 - val_mse: 0.2416\n",
            "Epoch 159/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2150 - mse: 0.2150 - val_loss: 0.2411 - val_mse: 0.2411\n",
            "Epoch 160/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2144 - mse: 0.2144 - val_loss: 0.2407 - val_mse: 0.2407\n",
            "Epoch 161/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2210 - mse: 0.2210 - val_loss: 0.2400 - val_mse: 0.2400\n",
            "Epoch 162/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2103 - mse: 0.2103 - val_loss: 0.2395 - val_mse: 0.2395\n",
            "Epoch 163/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2168 - mse: 0.2168 - val_loss: 0.2386 - val_mse: 0.2386\n",
            "Epoch 164/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2147 - mse: 0.2147 - val_loss: 0.2380 - val_mse: 0.2380\n",
            "Epoch 165/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2232 - mse: 0.2232 - val_loss: 0.2371 - val_mse: 0.2371\n",
            "Epoch 166/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.2154 - mse: 0.2154 - val_loss: 0.2365 - val_mse: 0.2365\n",
            "Epoch 167/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2201 - mse: 0.2201 - val_loss: 0.2357 - val_mse: 0.2357\n",
            "Epoch 168/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2122 - mse: 0.2122 - val_loss: 0.2355 - val_mse: 0.2355\n",
            "Epoch 169/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.2167 - mse: 0.2167 - val_loss: 0.2347 - val_mse: 0.2347\n",
            "Epoch 170/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2167 - mse: 0.2167 - val_loss: 0.2339 - val_mse: 0.2339\n",
            "Epoch 171/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2102 - mse: 0.2102 - val_loss: 0.2322 - val_mse: 0.2322\n",
            "Epoch 172/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.2138 - mse: 0.2138 - val_loss: 0.2313 - val_mse: 0.2313\n",
            "Epoch 173/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2096 - mse: 0.2096 - val_loss: 0.2307 - val_mse: 0.2307\n",
            "Epoch 174/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2300 - val_mse: 0.2300\n",
            "Epoch 175/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2030 - mse: 0.2030 - val_loss: 0.2310 - val_mse: 0.2310\n",
            "Epoch 176/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2063 - mse: 0.2063 - val_loss: 0.2291 - val_mse: 0.2291\n",
            "Epoch 177/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.2287 - val_mse: 0.2287\n",
            "Epoch 178/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2064 - mse: 0.2064 - val_loss: 0.2286 - val_mse: 0.2286\n",
            "Epoch 179/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2048 - mse: 0.2048 - val_loss: 0.2278 - val_mse: 0.2278\n",
            "Epoch 180/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2073 - mse: 0.2073 - val_loss: 0.2267 - val_mse: 0.2267\n",
            "Epoch 181/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2012 - mse: 0.2012 - val_loss: 0.2260 - val_mse: 0.2260\n",
            "Epoch 182/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2098 - mse: 0.2098 - val_loss: 0.2262 - val_mse: 0.2262\n",
            "Epoch 183/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2045 - mse: 0.2045 - val_loss: 0.2256 - val_mse: 0.2256\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 184/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.2012 - mse: 0.2012 - val_loss: 0.2244 - val_mse: 0.2244\n",
            "Epoch 185/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.2060 - mse: 0.2060 - val_loss: 0.2232 - val_mse: 0.2232\n",
            "Epoch 186/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2015 - mse: 0.2015 - val_loss: 0.2222 - val_mse: 0.2222\n",
            "Epoch 187/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2000 - mse: 0.2000 - val_loss: 0.2217 - val_mse: 0.2217\n",
            "Epoch 188/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2032 - mse: 0.2032 - val_loss: 0.2211 - val_mse: 0.2211\n",
            "Epoch 189/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1980 - mse: 0.1980 - val_loss: 0.2207 - val_mse: 0.2207\n",
            "Epoch 190/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2036 - mse: 0.2036 - val_loss: 0.2209 - val_mse: 0.2209\n",
            "Epoch 191/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1995 - mse: 0.1995 - val_loss: 0.2199 - val_mse: 0.2199\n",
            "Epoch 192/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1965 - mse: 0.1965 - val_loss: 0.2191 - val_mse: 0.2191\n",
            "Epoch 193/1000\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.1955 - mse: 0.1955 - val_loss: 0.2179 - val_mse: 0.2179\n",
            "Epoch 194/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2013 - mse: 0.2013 - val_loss: 0.2174 - val_mse: 0.2174\n",
            "Epoch 195/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1963 - mse: 0.1963 - val_loss: 0.2167 - val_mse: 0.2167\n",
            "Epoch 196/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1918 - mse: 0.1918 - val_loss: 0.2161 - val_mse: 0.2161\n",
            "Epoch 197/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2008 - mse: 0.2008 - val_loss: 0.2161 - val_mse: 0.2161\n",
            "Epoch 198/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1963 - mse: 0.1963 - val_loss: 0.2148 - val_mse: 0.2148\n",
            "Epoch 199/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1952 - mse: 0.1952 - val_loss: 0.2147 - val_mse: 0.2147\n",
            "Epoch 200/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.2147 - val_mse: 0.2147\n",
            "Epoch 201/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1913 - mse: 0.1913 - val_loss: 0.2136 - val_mse: 0.2136\n",
            "Epoch 202/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1942 - mse: 0.1942 - val_loss: 0.2137 - val_mse: 0.2137\n",
            "Epoch 203/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1924 - mse: 0.1924 - val_loss: 0.2126 - val_mse: 0.2126\n",
            "Epoch 204/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1906 - mse: 0.1906 - val_loss: 0.2129 - val_mse: 0.2129\n",
            "Epoch 205/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1920 - mse: 0.1920 - val_loss: 0.2118 - val_mse: 0.2118\n",
            "Epoch 206/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1933 - mse: 0.1933 - val_loss: 0.2119 - val_mse: 0.2119\n",
            "Epoch 207/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1859 - mse: 0.1859 - val_loss: 0.2111 - val_mse: 0.2111\n",
            "Epoch 208/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1874 - mse: 0.1874 - val_loss: 0.2101 - val_mse: 0.2101\n",
            "Epoch 209/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1900 - mse: 0.1900 - val_loss: 0.2095 - val_mse: 0.2095\n",
            "Epoch 210/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.2097 - val_mse: 0.2097\n",
            "Epoch 211/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1876 - mse: 0.1876 - val_loss: 0.2087 - val_mse: 0.2087\n",
            "Epoch 212/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1900 - mse: 0.1900 - val_loss: 0.2079 - val_mse: 0.2079\n",
            "Epoch 213/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.2073 - val_mse: 0.2073\n",
            "Epoch 214/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1809 - mse: 0.1809 - val_loss: 0.2066 - val_mse: 0.2066\n",
            "Epoch 215/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.2054 - val_mse: 0.2054\n",
            "Epoch 216/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1837 - mse: 0.1837 - val_loss: 0.2049 - val_mse: 0.2049\n",
            "Epoch 217/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1827 - mse: 0.1827 - val_loss: 0.2050 - val_mse: 0.2050\n",
            "Epoch 218/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1829 - mse: 0.1829 - val_loss: 0.2042 - val_mse: 0.2042\n",
            "Epoch 219/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.2041 - val_mse: 0.2041\n",
            "Epoch 220/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1860 - mse: 0.1860 - val_loss: 0.2036 - val_mse: 0.2036\n",
            "Epoch 221/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1793 - mse: 0.1793 - val_loss: 0.2031 - val_mse: 0.2031\n",
            "Epoch 222/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1790 - mse: 0.1790 - val_loss: 0.2026 - val_mse: 0.2026\n",
            "Epoch 223/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1826 - mse: 0.1826 - val_loss: 0.2031 - val_mse: 0.2031\n",
            "Epoch 224/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.2014 - val_mse: 0.2014\n",
            "Epoch 225/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1805 - mse: 0.1805 - val_loss: 0.2007 - val_mse: 0.2007\n",
            "Epoch 226/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1777 - mse: 0.1777 - val_loss: 0.2003 - val_mse: 0.2003\n",
            "Epoch 227/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1829 - mse: 0.1829 - val_loss: 0.2000 - val_mse: 0.2000\n",
            "Epoch 228/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1780 - mse: 0.1780 - val_loss: 0.1992 - val_mse: 0.1992\n",
            "Epoch 229/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.1988 - val_mse: 0.1988\n",
            "Epoch 230/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1842 - mse: 0.1842 - val_loss: 0.1980 - val_mse: 0.1980\n",
            "Epoch 231/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1759 - mse: 0.1759 - val_loss: 0.1974 - val_mse: 0.1974\n",
            "Epoch 232/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.1974 - val_mse: 0.1974\n",
            "Epoch 233/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1789 - mse: 0.1789 - val_loss: 0.1969 - val_mse: 0.1969\n",
            "Epoch 234/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1784 - mse: 0.1784 - val_loss: 0.1959 - val_mse: 0.1959\n",
            "Epoch 235/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1768 - mse: 0.1768 - val_loss: 0.1958 - val_mse: 0.1958\n",
            "Epoch 236/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1756 - mse: 0.1756 - val_loss: 0.1959 - val_mse: 0.1959\n",
            "Epoch 237/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1762 - mse: 0.1762 - val_loss: 0.1952 - val_mse: 0.1952\n",
            "Epoch 238/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1780 - mse: 0.1780 - val_loss: 0.1952 - val_mse: 0.1952\n",
            "Epoch 239/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1717 - mse: 0.1717 - val_loss: 0.1952 - val_mse: 0.1952\n",
            "Epoch 240/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.1938 - val_mse: 0.1938\n",
            "Epoch 241/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1764 - mse: 0.1764 - val_loss: 0.1938 - val_mse: 0.1938\n",
            "Epoch 242/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1743 - mse: 0.1743 - val_loss: 0.1932 - val_mse: 0.1932\n",
            "Epoch 243/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1720 - mse: 0.1720 - val_loss: 0.1931 - val_mse: 0.1931\n",
            "Epoch 244/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.1923 - val_mse: 0.1923\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 245/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1710 - mse: 0.1710 - val_loss: 0.1925 - val_mse: 0.1925\n",
            "Epoch 246/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1725 - mse: 0.1725 - val_loss: 0.1918 - val_mse: 0.1918\n",
            "Epoch 247/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.1904 - val_mse: 0.1904\n",
            "Epoch 248/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1692 - mse: 0.1692 - val_loss: 0.1906 - val_mse: 0.1906\n",
            "Epoch 249/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1706 - mse: 0.1706 - val_loss: 0.1903 - val_mse: 0.1903\n",
            "Epoch 250/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1729 - mse: 0.1729 - val_loss: 0.1895 - val_mse: 0.1895\n",
            "Epoch 251/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1741 - mse: 0.1741 - val_loss: 0.1890 - val_mse: 0.1890\n",
            "Epoch 252/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1701 - mse: 0.1701 - val_loss: 0.1886 - val_mse: 0.1886\n",
            "Epoch 253/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1694 - mse: 0.1694 - val_loss: 0.1879 - val_mse: 0.1879\n",
            "Epoch 254/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1746 - mse: 0.1746 - val_loss: 0.1876 - val_mse: 0.1876\n",
            "Epoch 255/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1655 - mse: 0.1655 - val_loss: 0.1875 - val_mse: 0.1875\n",
            "Epoch 256/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1688 - mse: 0.1688 - val_loss: 0.1869 - val_mse: 0.1869\n",
            "Epoch 257/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1651 - mse: 0.1651 - val_loss: 0.1870 - val_mse: 0.1870\n",
            "Epoch 258/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1662 - mse: 0.1662 - val_loss: 0.1865 - val_mse: 0.1865\n",
            "Epoch 259/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1660 - mse: 0.1660 - val_loss: 0.1860 - val_mse: 0.1860\n",
            "Epoch 260/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1667 - mse: 0.1667 - val_loss: 0.1862 - val_mse: 0.1862\n",
            "Epoch 261/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1632 - mse: 0.1632 - val_loss: 0.1846 - val_mse: 0.1846\n",
            "Epoch 262/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1648 - mse: 0.1648 - val_loss: 0.1844 - val_mse: 0.1844\n",
            "Epoch 263/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1630 - mse: 0.1630 - val_loss: 0.1837 - val_mse: 0.1837\n",
            "Epoch 264/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1661 - mse: 0.1661 - val_loss: 0.1831 - val_mse: 0.1831\n",
            "Epoch 265/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1638 - mse: 0.1638 - val_loss: 0.1836 - val_mse: 0.1836\n",
            "Epoch 266/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1680 - mse: 0.1680 - val_loss: 0.1834 - val_mse: 0.1834\n",
            "Epoch 267/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1655 - mse: 0.1655 - val_loss: 0.1831 - val_mse: 0.1831\n",
            "Epoch 268/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1672 - mse: 0.1672 - val_loss: 0.1822 - val_mse: 0.1822\n",
            "Epoch 269/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1618 - mse: 0.1618 - val_loss: 0.1819 - val_mse: 0.1819\n",
            "Epoch 270/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1608 - mse: 0.1608 - val_loss: 0.1812 - val_mse: 0.1812\n",
            "Epoch 271/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1650 - mse: 0.1650 - val_loss: 0.1816 - val_mse: 0.1816\n",
            "Epoch 272/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1634 - mse: 0.1634 - val_loss: 0.1811 - val_mse: 0.1811\n",
            "Epoch 273/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1602 - mse: 0.1602 - val_loss: 0.1815 - val_mse: 0.1815\n",
            "Epoch 274/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1609 - mse: 0.1609 - val_loss: 0.1802 - val_mse: 0.1802\n",
            "Epoch 275/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1646 - mse: 0.1646 - val_loss: 0.1796 - val_mse: 0.1796\n",
            "Epoch 276/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1594 - mse: 0.1594 - val_loss: 0.1795 - val_mse: 0.1795\n",
            "Epoch 277/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1639 - mse: 0.1639 - val_loss: 0.1792 - val_mse: 0.1792\n",
            "Epoch 278/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1634 - mse: 0.1634 - val_loss: 0.1787 - val_mse: 0.1787\n",
            "Epoch 279/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1599 - mse: 0.1599 - val_loss: 0.1786 - val_mse: 0.1786\n",
            "Epoch 280/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1627 - mse: 0.1627 - val_loss: 0.1784 - val_mse: 0.1784\n",
            "Epoch 281/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1623 - mse: 0.1623 - val_loss: 0.1781 - val_mse: 0.1781\n",
            "Epoch 282/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1620 - mse: 0.1620 - val_loss: 0.1778 - val_mse: 0.1778\n",
            "Epoch 283/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1617 - mse: 0.1617 - val_loss: 0.1776 - val_mse: 0.1776\n",
            "Epoch 284/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1576 - mse: 0.1576 - val_loss: 0.1769 - val_mse: 0.1769\n",
            "Epoch 285/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1588 - mse: 0.1588 - val_loss: 0.1775 - val_mse: 0.1775\n",
            "Epoch 286/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1582 - mse: 0.1582 - val_loss: 0.1771 - val_mse: 0.1771\n",
            "Epoch 287/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1590 - mse: 0.1590 - val_loss: 0.1780 - val_mse: 0.1780\n",
            "Epoch 288/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1621 - mse: 0.1621 - val_loss: 0.1761 - val_mse: 0.1761\n",
            "Epoch 289/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1760 - val_mse: 0.1760\n",
            "Epoch 290/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1569 - mse: 0.1569 - val_loss: 0.1765 - val_mse: 0.1765\n",
            "Epoch 291/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1597 - mse: 0.1597 - val_loss: 0.1763 - val_mse: 0.1763\n",
            "Epoch 292/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1583 - mse: 0.1583 - val_loss: 0.1753 - val_mse: 0.1753\n",
            "Epoch 293/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1548 - mse: 0.1548 - val_loss: 0.1747 - val_mse: 0.1747\n",
            "Epoch 294/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1595 - mse: 0.1595 - val_loss: 0.1747 - val_mse: 0.1747\n",
            "Epoch 295/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1587 - mse: 0.1587 - val_loss: 0.1746 - val_mse: 0.1746\n",
            "Epoch 296/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1576 - mse: 0.1576 - val_loss: 0.1745 - val_mse: 0.1745\n",
            "Epoch 297/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1589 - mse: 0.1589 - val_loss: 0.1733 - val_mse: 0.1733\n",
            "Epoch 298/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1740 - val_mse: 0.1740\n",
            "Epoch 299/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1543 - mse: 0.1543 - val_loss: 0.1734 - val_mse: 0.1734\n",
            "Epoch 300/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1567 - mse: 0.1567 - val_loss: 0.1730 - val_mse: 0.1730\n",
            "Epoch 301/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1541 - mse: 0.1541 - val_loss: 0.1725 - val_mse: 0.1725\n",
            "Epoch 302/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1527 - mse: 0.1527 - val_loss: 0.1723 - val_mse: 0.1723\n",
            "Epoch 303/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1567 - mse: 0.1567 - val_loss: 0.1723 - val_mse: 0.1723\n",
            "Epoch 304/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.1722 - val_mse: 0.1722\n",
            "Epoch 305/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1521 - mse: 0.1521 - val_loss: 0.1718 - val_mse: 0.1718\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 306/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1543 - mse: 0.1543 - val_loss: 0.1720 - val_mse: 0.1720\n",
            "Epoch 307/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1510 - mse: 0.1510 - val_loss: 0.1720 - val_mse: 0.1720\n",
            "Epoch 308/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.1720 - val_mse: 0.1720\n",
            "Epoch 309/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1520 - mse: 0.1520 - val_loss: 0.1714 - val_mse: 0.1714\n",
            "Epoch 310/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1515 - mse: 0.1515 - val_loss: 0.1710 - val_mse: 0.1710\n",
            "Epoch 311/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1533 - mse: 0.1533 - val_loss: 0.1708 - val_mse: 0.1708\n",
            "Epoch 312/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1548 - mse: 0.1548 - val_loss: 0.1706 - val_mse: 0.1706\n",
            "Epoch 313/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1504 - mse: 0.1504 - val_loss: 0.1705 - val_mse: 0.1705\n",
            "Epoch 314/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1506 - mse: 0.1506 - val_loss: 0.1701 - val_mse: 0.1701\n",
            "Epoch 315/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1529 - mse: 0.1529 - val_loss: 0.1701 - val_mse: 0.1701\n",
            "Epoch 316/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 0.1696 - val_mse: 0.1696\n",
            "Epoch 317/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1514 - mse: 0.1514 - val_loss: 0.1699 - val_mse: 0.1699\n",
            "Epoch 318/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1484 - mse: 0.1484 - val_loss: 0.1695 - val_mse: 0.1695\n",
            "Epoch 319/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1509 - mse: 0.1509 - val_loss: 0.1699 - val_mse: 0.1699\n",
            "Epoch 320/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1535 - mse: 0.1535 - val_loss: 0.1691 - val_mse: 0.1691\n",
            "Epoch 321/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1494 - mse: 0.1494 - val_loss: 0.1689 - val_mse: 0.1689\n",
            "Epoch 322/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1496 - mse: 0.1496 - val_loss: 0.1685 - val_mse: 0.1685\n",
            "Epoch 323/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1507 - mse: 0.1507 - val_loss: 0.1689 - val_mse: 0.1689\n",
            "Epoch 324/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1506 - mse: 0.1506 - val_loss: 0.1688 - val_mse: 0.1688\n",
            "Epoch 325/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1501 - mse: 0.1501 - val_loss: 0.1686 - val_mse: 0.1686\n",
            "Epoch 326/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1464 - mse: 0.1464 - val_loss: 0.1689 - val_mse: 0.1689\n",
            "Epoch 327/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1482 - mse: 0.1482 - val_loss: 0.1686 - val_mse: 0.1686\n",
            "Epoch 328/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1489 - mse: 0.1489 - val_loss: 0.1687 - val_mse: 0.1687\n",
            "Epoch 329/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1477 - mse: 0.1477 - val_loss: 0.1678 - val_mse: 0.1678\n",
            "Epoch 330/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1473 - mse: 0.1473 - val_loss: 0.1678 - val_mse: 0.1678\n",
            "Epoch 331/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1478 - mse: 0.1478 - val_loss: 0.1675 - val_mse: 0.1675\n",
            "Epoch 332/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1518 - mse: 0.1518 - val_loss: 0.1670 - val_mse: 0.1670\n",
            "Epoch 333/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1501 - mse: 0.1501 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "Epoch 334/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1475 - mse: 0.1475 - val_loss: 0.1667 - val_mse: 0.1667\n",
            "Epoch 335/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1456 - mse: 0.1456 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "Epoch 336/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.1663 - val_mse: 0.1663\n",
            "Epoch 337/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1462 - mse: 0.1462 - val_loss: 0.1657 - val_mse: 0.1657\n",
            "Epoch 338/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1492 - mse: 0.1492 - val_loss: 0.1659 - val_mse: 0.1659\n",
            "Epoch 339/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1474 - mse: 0.1474 - val_loss: 0.1662 - val_mse: 0.1662\n",
            "Epoch 340/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1451 - mse: 0.1451 - val_loss: 0.1648 - val_mse: 0.1648\n",
            "Epoch 341/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1413 - mse: 0.1413 - val_loss: 0.1644 - val_mse: 0.1644\n",
            "Epoch 342/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1482 - mse: 0.1482 - val_loss: 0.1640 - val_mse: 0.1640\n",
            "Epoch 343/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1423 - mse: 0.1423 - val_loss: 0.1641 - val_mse: 0.1641\n",
            "Epoch 344/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1486 - mse: 0.1486 - val_loss: 0.1642 - val_mse: 0.1642\n",
            "Epoch 345/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1434 - mse: 0.1434 - val_loss: 0.1643 - val_mse: 0.1643\n",
            "Epoch 346/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1428 - mse: 0.1428 - val_loss: 0.1639 - val_mse: 0.1639\n",
            "Epoch 347/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1454 - mse: 0.1454 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "Epoch 348/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1452 - mse: 0.1452 - val_loss: 0.1631 - val_mse: 0.1631\n",
            "Epoch 349/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1435 - mse: 0.1435 - val_loss: 0.1628 - val_mse: 0.1628\n",
            "Epoch 350/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1434 - mse: 0.1434 - val_loss: 0.1629 - val_mse: 0.1629\n",
            "Epoch 351/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1445 - mse: 0.1445 - val_loss: 0.1627 - val_mse: 0.1627\n",
            "Epoch 352/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1440 - mse: 0.1440 - val_loss: 0.1622 - val_mse: 0.1622\n",
            "Epoch 353/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1399 - mse: 0.1399 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 354/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1435 - mse: 0.1435 - val_loss: 0.1620 - val_mse: 0.1620\n",
            "Epoch 355/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1389 - mse: 0.1389 - val_loss: 0.1628 - val_mse: 0.1628\n",
            "Epoch 356/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1423 - mse: 0.1423 - val_loss: 0.1627 - val_mse: 0.1627\n",
            "Epoch 357/1000\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.1432 - mse: 0.1432 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "Epoch 358/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1437 - mse: 0.1437 - val_loss: 0.1620 - val_mse: 0.1620\n",
            "Epoch 359/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1439 - mse: 0.1439 - val_loss: 0.1622 - val_mse: 0.1622\n",
            "Epoch 360/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1450 - mse: 0.1450 - val_loss: 0.1614 - val_mse: 0.1614\n",
            "Epoch 361/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1407 - mse: 0.1407 - val_loss: 0.1609 - val_mse: 0.1609\n",
            "Epoch 362/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.1612 - val_mse: 0.1612\n",
            "Epoch 363/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1417 - mse: 0.1417 - val_loss: 0.1614 - val_mse: 0.1614\n",
            "Epoch 364/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1414 - mse: 0.1414 - val_loss: 0.1613 - val_mse: 0.1613\n",
            "Epoch 365/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1401 - mse: 0.1401 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 366/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1404 - mse: 0.1404 - val_loss: 0.1617 - val_mse: 0.1617\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 367/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1385 - mse: 0.1385 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 368/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1409 - mse: 0.1409 - val_loss: 0.1613 - val_mse: 0.1613\n",
            "Epoch 369/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1412 - mse: 0.1412 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 370/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1396 - mse: 0.1396 - val_loss: 0.1610 - val_mse: 0.1610\n",
            "Epoch 371/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1410 - mse: 0.1410 - val_loss: 0.1616 - val_mse: 0.1616\n",
            "Epoch 372/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1391 - mse: 0.1391 - val_loss: 0.1609 - val_mse: 0.1609\n",
            "Epoch 373/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1388 - mse: 0.1388 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 374/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1413 - mse: 0.1413 - val_loss: 0.1603 - val_mse: 0.1603\n",
            "Epoch 375/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1414 - mse: 0.1414 - val_loss: 0.1604 - val_mse: 0.1604\n",
            "Epoch 376/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1371 - mse: 0.1371 - val_loss: 0.1606 - val_mse: 0.1606\n",
            "Epoch 377/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1398 - mse: 0.1398 - val_loss: 0.1604 - val_mse: 0.1604\n",
            "Epoch 378/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1386 - mse: 0.1386 - val_loss: 0.1605 - val_mse: 0.1605\n",
            "Epoch 379/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1354 - mse: 0.1354 - val_loss: 0.1600 - val_mse: 0.1600\n",
            "Epoch 380/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1409 - mse: 0.1409 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 381/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1373 - mse: 0.1373 - val_loss: 0.1600 - val_mse: 0.1600\n",
            "Epoch 382/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1392 - mse: 0.1392 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 383/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1399 - mse: 0.1399 - val_loss: 0.1604 - val_mse: 0.1604\n",
            "Epoch 384/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1388 - mse: 0.1388 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "Epoch 385/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1362 - mse: 0.1362 - val_loss: 0.1598 - val_mse: 0.1598\n",
            "Epoch 386/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1391 - mse: 0.1391 - val_loss: 0.1596 - val_mse: 0.1596\n",
            "Epoch 387/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1376 - mse: 0.1376 - val_loss: 0.1589 - val_mse: 0.1589\n",
            "Epoch 388/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1365 - mse: 0.1365 - val_loss: 0.1588 - val_mse: 0.1588\n",
            "Epoch 389/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1345 - mse: 0.1345 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 390/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1381 - mse: 0.1381 - val_loss: 0.1590 - val_mse: 0.1590\n",
            "Epoch 391/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1349 - mse: 0.1349 - val_loss: 0.1590 - val_mse: 0.1590\n",
            "Epoch 392/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1391 - mse: 0.1391 - val_loss: 0.1593 - val_mse: 0.1593\n",
            "Epoch 393/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1360 - mse: 0.1360 - val_loss: 0.1594 - val_mse: 0.1594\n",
            "Epoch 394/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1348 - mse: 0.1348 - val_loss: 0.1597 - val_mse: 0.1597\n",
            "Epoch 395/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1355 - mse: 0.1355 - val_loss: 0.1589 - val_mse: 0.1589\n",
            "Epoch 396/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1381 - mse: 0.1381 - val_loss: 0.1590 - val_mse: 0.1590\n",
            "Epoch 397/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1351 - mse: 0.1351 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "Epoch 398/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1374 - mse: 0.1374 - val_loss: 0.1587 - val_mse: 0.1587\n",
            "Epoch 399/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1356 - mse: 0.1356 - val_loss: 0.1590 - val_mse: 0.1590\n",
            "Epoch 400/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1351 - mse: 0.1351 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 401/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1331 - mse: 0.1331 - val_loss: 0.1590 - val_mse: 0.1590\n",
            "Epoch 402/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1369 - mse: 0.1369 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "Epoch 403/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1346 - mse: 0.1346 - val_loss: 0.1580 - val_mse: 0.1580\n",
            "Epoch 404/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1370 - mse: 0.1370 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "Epoch 405/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1343 - mse: 0.1343 - val_loss: 0.1580 - val_mse: 0.1580\n",
            "Epoch 406/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1355 - mse: 0.1355 - val_loss: 0.1579 - val_mse: 0.1579\n",
            "Epoch 407/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1360 - mse: 0.1360 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "Epoch 408/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1345 - mse: 0.1345 - val_loss: 0.1580 - val_mse: 0.1580\n",
            "Epoch 409/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1325 - mse: 0.1325 - val_loss: 0.1579 - val_mse: 0.1579\n",
            "Epoch 410/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1331 - mse: 0.1331 - val_loss: 0.1576 - val_mse: 0.1576\n",
            "Epoch 411/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1340 - mse: 0.1340 - val_loss: 0.1574 - val_mse: 0.1574\n",
            "Epoch 412/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1369 - mse: 0.1369 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 413/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1337 - mse: 0.1337 - val_loss: 0.1567 - val_mse: 0.1567\n",
            "Epoch 414/1000\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.1355 - mse: 0.1355 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 415/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1336 - mse: 0.1336 - val_loss: 0.1563 - val_mse: 0.1563\n",
            "Epoch 416/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1332 - mse: 0.1332 - val_loss: 0.1566 - val_mse: 0.1566\n",
            "Epoch 417/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1339 - mse: 0.1339 - val_loss: 0.1564 - val_mse: 0.1564\n",
            "Epoch 418/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1340 - mse: 0.1340 - val_loss: 0.1570 - val_mse: 0.1570\n",
            "Epoch 419/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1323 - mse: 0.1323 - val_loss: 0.1563 - val_mse: 0.1563\n",
            "Epoch 420/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1334 - mse: 0.1334 - val_loss: 0.1565 - val_mse: 0.1565\n",
            "Epoch 421/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1347 - mse: 0.1347 - val_loss: 0.1565 - val_mse: 0.1565\n",
            "Epoch 422/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1302 - mse: 0.1302 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 423/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1342 - mse: 0.1342 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "Epoch 424/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1320 - mse: 0.1320 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 425/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1347 - mse: 0.1347 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "Epoch 426/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1312 - mse: 0.1312 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "Epoch 427/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1320 - mse: 0.1320 - val_loss: 0.1563 - val_mse: 0.1563\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 428/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1298 - mse: 0.1298 - val_loss: 0.1560 - val_mse: 0.1560\n",
            "Epoch 429/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1351 - mse: 0.1351 - val_loss: 0.1553 - val_mse: 0.1553\n",
            "Epoch 430/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1554 - val_mse: 0.1554\n",
            "Epoch 431/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1553 - val_mse: 0.1553\n",
            "Epoch 432/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1333 - mse: 0.1333 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 433/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1314 - mse: 0.1314 - val_loss: 0.1551 - val_mse: 0.1551\n",
            "Epoch 434/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1333 - mse: 0.1333 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "Epoch 435/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1330 - mse: 0.1330 - val_loss: 0.1553 - val_mse: 0.1553\n",
            "Epoch 436/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1555 - val_mse: 0.1555\n",
            "Epoch 437/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1302 - mse: 0.1302 - val_loss: 0.1556 - val_mse: 0.1556\n",
            "Epoch 438/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1548 - val_mse: 0.1548\n",
            "Epoch 439/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1310 - mse: 0.1310 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "Epoch 440/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1341 - mse: 0.1341 - val_loss: 0.1551 - val_mse: 0.1551\n",
            "Epoch 441/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1303 - mse: 0.1303 - val_loss: 0.1551 - val_mse: 0.1551\n",
            "Epoch 442/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1267 - mse: 0.1267 - val_loss: 0.1550 - val_mse: 0.1550\n",
            "Epoch 443/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1334 - mse: 0.1334 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "Epoch 444/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1276 - mse: 0.1276 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "Epoch 445/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1312 - mse: 0.1312 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "Epoch 446/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1311 - mse: 0.1311 - val_loss: 0.1547 - val_mse: 0.1547\n",
            "Epoch 447/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1310 - mse: 0.1310 - val_loss: 0.1545 - val_mse: 0.1545\n",
            "Epoch 448/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1281 - mse: 0.1281 - val_loss: 0.1541 - val_mse: 0.1541\n",
            "Epoch 449/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1287 - mse: 0.1287 - val_loss: 0.1550 - val_mse: 0.1550\n",
            "Epoch 450/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1275 - mse: 0.1275 - val_loss: 0.1548 - val_mse: 0.1548\n",
            "Epoch 451/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1275 - mse: 0.1275 - val_loss: 0.1550 - val_mse: 0.1550\n",
            "Epoch 452/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1287 - mse: 0.1287 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "Epoch 453/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1246 - mse: 0.1246 - val_loss: 0.1543 - val_mse: 0.1543\n",
            "Epoch 454/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1297 - mse: 0.1297 - val_loss: 0.1537 - val_mse: 0.1537\n",
            "Epoch 455/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1270 - mse: 0.1270 - val_loss: 0.1536 - val_mse: 0.1536\n",
            "Epoch 456/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1539 - val_mse: 0.1539\n",
            "Epoch 457/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1269 - mse: 0.1269 - val_loss: 0.1543 - val_mse: 0.1543\n",
            "Epoch 458/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1270 - mse: 0.1270 - val_loss: 0.1543 - val_mse: 0.1543\n",
            "Epoch 459/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1276 - mse: 0.1276 - val_loss: 0.1543 - val_mse: 0.1543\n",
            "Epoch 460/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1301 - mse: 0.1301 - val_loss: 0.1536 - val_mse: 0.1536\n",
            "Epoch 461/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1234 - mse: 0.1234 - val_loss: 0.1544 - val_mse: 0.1544\n",
            "Epoch 462/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1282 - mse: 0.1282 - val_loss: 0.1534 - val_mse: 0.1534\n",
            "Epoch 463/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1269 - mse: 0.1269 - val_loss: 0.1533 - val_mse: 0.1533\n",
            "Epoch 464/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1256 - mse: 0.1256 - val_loss: 0.1532 - val_mse: 0.1532\n",
            "Epoch 465/1000\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.1291 - mse: 0.1291 - val_loss: 0.1534 - val_mse: 0.1534\n",
            "Epoch 466/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1271 - mse: 0.1271 - val_loss: 0.1538 - val_mse: 0.1538\n",
            "Epoch 467/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1274 - mse: 0.1274 - val_loss: 0.1539 - val_mse: 0.1539\n",
            "Epoch 468/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1247 - mse: 0.1247 - val_loss: 0.1535 - val_mse: 0.1535\n",
            "Epoch 469/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1257 - mse: 0.1257 - val_loss: 0.1535 - val_mse: 0.1535\n",
            "Epoch 470/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1269 - mse: 0.1269 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 471/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1278 - mse: 0.1278 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 472/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1289 - mse: 0.1289 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 473/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1228 - mse: 0.1228 - val_loss: 0.1528 - val_mse: 0.1528\n",
            "Epoch 474/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1251 - mse: 0.1251 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 475/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1263 - mse: 0.1263 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 476/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1261 - mse: 0.1261 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 477/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1237 - mse: 0.1237 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 478/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1229 - mse: 0.1229 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 479/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1254 - mse: 0.1254 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 480/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1263 - mse: 0.1263 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 481/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1250 - mse: 0.1250 - val_loss: 0.1521 - val_mse: 0.1521\n",
            "Epoch 482/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1261 - mse: 0.1261 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 483/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1261 - mse: 0.1261 - val_loss: 0.1520 - val_mse: 0.1520\n",
            "Epoch 484/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1252 - mse: 0.1252 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 485/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1256 - mse: 0.1256 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 486/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1253 - mse: 0.1253 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "Epoch 487/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1223 - mse: 0.1223 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 488/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1260 - mse: 0.1260 - val_loss: 0.1516 - val_mse: 0.1516\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 489/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1235 - mse: 0.1235 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 490/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1260 - mse: 0.1260 - val_loss: 0.1523 - val_mse: 0.1523\n",
            "Epoch 491/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1268 - mse: 0.1268 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 492/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1242 - mse: 0.1242 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 493/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1275 - mse: 0.1275 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 494/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1237 - mse: 0.1237 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 495/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1230 - mse: 0.1230 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 496/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1286 - mse: 0.1286 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "Epoch 497/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1256 - mse: 0.1256 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "Epoch 498/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1225 - mse: 0.1225 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "Epoch 499/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1228 - mse: 0.1228 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 500/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1231 - mse: 0.1231 - val_loss: 0.1510 - val_mse: 0.1510\n",
            "Epoch 501/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1253 - mse: 0.1253 - val_loss: 0.1507 - val_mse: 0.1507\n",
            "Epoch 502/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1235 - mse: 0.1235 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "Epoch 503/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1243 - mse: 0.1243 - val_loss: 0.1505 - val_mse: 0.1505\n",
            "Epoch 504/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1270 - mse: 0.1270 - val_loss: 0.1510 - val_mse: 0.1510\n",
            "Epoch 505/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1217 - mse: 0.1217 - val_loss: 0.1511 - val_mse: 0.1511\n",
            "Epoch 506/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1209 - mse: 0.1209 - val_loss: 0.1506 - val_mse: 0.1506\n",
            "Epoch 507/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1228 - mse: 0.1228 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "Epoch 508/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1240 - mse: 0.1240 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 509/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1226 - mse: 0.1226 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 510/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1236 - mse: 0.1236 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 511/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1242 - mse: 0.1242 - val_loss: 0.1496 - val_mse: 0.1496\n",
            "Epoch 512/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1230 - mse: 0.1230 - val_loss: 0.1496 - val_mse: 0.1496\n",
            "Epoch 513/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1253 - mse: 0.1253 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 514/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1219 - mse: 0.1219 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 515/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1244 - mse: 0.1244 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 516/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1196 - mse: 0.1196 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "Epoch 517/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1185 - mse: 0.1185 - val_loss: 0.1497 - val_mse: 0.1497\n",
            "Epoch 518/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1255 - mse: 0.1255 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 519/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1241 - mse: 0.1241 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "Epoch 520/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1246 - mse: 0.1246 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 521/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1231 - mse: 0.1231 - val_loss: 0.1498 - val_mse: 0.1498\n",
            "Epoch 522/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1230 - mse: 0.1230 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "Epoch 523/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1211 - mse: 0.1211 - val_loss: 0.1496 - val_mse: 0.1496\n",
            "Epoch 524/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1227 - mse: 0.1227 - val_loss: 0.1497 - val_mse: 0.1497\n",
            "Epoch 525/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1210 - mse: 0.1210 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 526/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1209 - mse: 0.1209 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 527/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1217 - mse: 0.1217 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 528/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1244 - mse: 0.1244 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "Epoch 529/1000\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.1244 - mse: 0.1244 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "Epoch 530/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1220 - mse: 0.1220 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 531/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1231 - mse: 0.1231 - val_loss: 0.1487 - val_mse: 0.1487\n",
            "Epoch 532/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1197 - mse: 0.1197 - val_loss: 0.1490 - val_mse: 0.1490\n",
            "Epoch 533/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1201 - mse: 0.1201 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 534/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1240 - mse: 0.1240 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 535/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1188 - mse: 0.1188 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 536/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1201 - mse: 0.1201 - val_loss: 0.1483 - val_mse: 0.1483\n",
            "Epoch 537/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1192 - mse: 0.1192 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "Epoch 538/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1198 - mse: 0.1198 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "Epoch 539/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1201 - mse: 0.1201 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "Epoch 540/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1185 - mse: 0.1185 - val_loss: 0.1487 - val_mse: 0.1487\n",
            "Epoch 541/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1212 - mse: 0.1212 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "Epoch 542/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1188 - mse: 0.1188 - val_loss: 0.1484 - val_mse: 0.1484\n",
            "Epoch 543/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1202 - mse: 0.1202 - val_loss: 0.1484 - val_mse: 0.1484\n",
            "Epoch 544/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1229 - mse: 0.1229 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "Epoch 545/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1170 - mse: 0.1170 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 546/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1207 - mse: 0.1207 - val_loss: 0.1481 - val_mse: 0.1481\n",
            "Epoch 547/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1193 - mse: 0.1193 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "Epoch 548/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1175 - mse: 0.1175 - val_loss: 0.1482 - val_mse: 0.1482\n",
            "Epoch 549/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1195 - mse: 0.1195 - val_loss: 0.1482 - val_mse: 0.1482\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 550/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1204 - mse: 0.1204 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 551/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1195 - mse: 0.1195 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 552/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1203 - mse: 0.1203 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "Epoch 553/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1179 - mse: 0.1179 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 554/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1196 - mse: 0.1196 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 555/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1170 - mse: 0.1170 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 556/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1180 - mse: 0.1180 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 557/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1176 - mse: 0.1176 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 558/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1203 - mse: 0.1203 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "Epoch 559/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1166 - mse: 0.1166 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "Epoch 560/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1158 - mse: 0.1158 - val_loss: 0.1481 - val_mse: 0.1481\n",
            "Epoch 561/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1214 - mse: 0.1214 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 562/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1172 - mse: 0.1172 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "Epoch 563/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1212 - mse: 0.1212 - val_loss: 0.1474 - val_mse: 0.1474\n",
            "Epoch 564/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1182 - mse: 0.1182 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 565/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1158 - mse: 0.1158 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 566/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1175 - mse: 0.1175 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 567/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.1166 - mse: 0.1166 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 568/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1223 - mse: 0.1223 - val_loss: 0.1475 - val_mse: 0.1475\n",
            "Epoch 569/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1151 - mse: 0.1151 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 570/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1172 - mse: 0.1172 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 571/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1156 - mse: 0.1156 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 572/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1183 - mse: 0.1183 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "Epoch 573/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1162 - mse: 0.1162 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 574/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1193 - mse: 0.1193 - val_loss: 0.1475 - val_mse: 0.1475\n",
            "Epoch 575/1000\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1177 - mse: 0.1177 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 576/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1186 - mse: 0.1186 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 577/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1178 - mse: 0.1178 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 578/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1173 - mse: 0.1173 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 579/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1205 - mse: 0.1205 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 580/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1167 - mse: 0.1167 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 581/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1208 - mse: 0.1208 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 582/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1167 - mse: 0.1167 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "Epoch 583/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1157 - mse: 0.1157 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 584/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1184 - mse: 0.1184 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 585/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1168 - mse: 0.1168 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 586/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1167 - mse: 0.1167 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 587/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1167 - mse: 0.1167 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "Epoch 588/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1172 - mse: 0.1172 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "Epoch 589/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1183 - mse: 0.1183 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 590/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1161 - mse: 0.1161 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "Epoch 591/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1166 - mse: 0.1166 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 592/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1186 - mse: 0.1186 - val_loss: 0.1474 - val_mse: 0.1474\n",
            "Epoch 593/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1152 - mse: 0.1152 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "Epoch 594/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1170 - mse: 0.1170 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 595/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1169 - mse: 0.1169 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 596/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1159 - mse: 0.1159 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 597/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1163 - mse: 0.1163 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 598/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1119 - mse: 0.1119 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 599/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1181 - mse: 0.1181 - val_loss: 0.1466 - val_mse: 0.1466\n",
            "Epoch 600/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1182 - mse: 0.1182 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 601/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1149 - mse: 0.1149 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 602/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1144 - mse: 0.1144 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 603/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1147 - mse: 0.1147 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 604/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1156 - mse: 0.1156 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 605/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1144 - mse: 0.1144 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 606/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1134 - mse: 0.1134 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 607/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 608/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 609/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1137 - mse: 0.1137 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 610/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1141 - mse: 0.1141 - val_loss: 0.1461 - val_mse: 0.1461\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 611/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1196 - mse: 0.1196 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 612/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1141 - mse: 0.1141 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 613/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1163 - mse: 0.1163 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 614/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1137 - mse: 0.1137 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 615/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1129 - mse: 0.1129 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 616/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1094 - mse: 0.1094 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 617/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1117 - mse: 0.1117 - val_loss: 0.1463 - val_mse: 0.1463\n",
            "Epoch 618/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1117 - mse: 0.1117 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 619/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1160 - mse: 0.1160 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 620/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 621/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1138 - mse: 0.1138 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 622/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1138 - mse: 0.1138 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 623/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1149 - mse: 0.1149 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "Epoch 624/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1142 - mse: 0.1142 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "Epoch 625/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1125 - mse: 0.1125 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 626/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1147 - mse: 0.1147 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 627/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1125 - mse: 0.1125 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 628/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1137 - mse: 0.1137 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 629/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 630/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1120 - mse: 0.1120 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 631/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 632/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1115 - mse: 0.1115 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 633/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1109 - mse: 0.1109 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 634/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1133 - mse: 0.1133 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 635/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1136 - mse: 0.1136 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 636/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1138 - mse: 0.1138 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "Epoch 637/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1115 - mse: 0.1115 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 638/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1094 - mse: 0.1094 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 639/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1110 - mse: 0.1110 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 640/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1124 - mse: 0.1124 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 641/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1155 - mse: 0.1155 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 642/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1129 - mse: 0.1129 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 643/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1145 - mse: 0.1145 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 644/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 645/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1129 - mse: 0.1129 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 646/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1162 - mse: 0.1162 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 647/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1128 - mse: 0.1128 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 648/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1120 - mse: 0.1120 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 649/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1106 - mse: 0.1106 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 650/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1130 - mse: 0.1130 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 651/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1085 - mse: 0.1085 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 652/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1105 - mse: 0.1105 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 653/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1157 - mse: 0.1157 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 654/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1155 - mse: 0.1155 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "Epoch 655/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1109 - mse: 0.1109 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "Epoch 656/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1110 - mse: 0.1110 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 657/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1121 - mse: 0.1121 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 658/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1136 - mse: 0.1136 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "Epoch 659/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "Epoch 660/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1110 - mse: 0.1110 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "Epoch 661/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1093 - mse: 0.1093 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 662/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1128 - mse: 0.1128 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 663/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1106 - mse: 0.1106 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 664/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 665/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1097 - mse: 0.1097 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 666/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1073 - mse: 0.1073 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 667/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1107 - mse: 0.1107 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 668/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1136 - mse: 0.1136 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 669/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1105 - mse: 0.1105 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 670/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1130 - mse: 0.1130 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 671/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1121 - mse: 0.1121 - val_loss: 0.1449 - val_mse: 0.1449\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 672/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1098 - mse: 0.1098 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 673/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1116 - mse: 0.1116 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 674/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1124 - mse: 0.1124 - val_loss: 0.1447 - val_mse: 0.1447\n",
            "Epoch 675/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1135 - mse: 0.1135 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 676/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1083 - mse: 0.1083 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 677/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1089 - mse: 0.1089 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 678/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1087 - mse: 0.1087 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 679/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 680/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1109 - mse: 0.1109 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 681/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1113 - mse: 0.1113 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 682/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1118 - mse: 0.1118 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 683/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1094 - mse: 0.1094 - val_loss: 0.1440 - val_mse: 0.1440\n",
            "Epoch 684/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1095 - mse: 0.1095 - val_loss: 0.1447 - val_mse: 0.1447\n",
            "Epoch 685/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1101 - mse: 0.1101 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 686/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1122 - mse: 0.1122 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 687/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1099 - mse: 0.1099 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 688/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1098 - mse: 0.1098 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 689/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1097 - mse: 0.1097 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 690/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1441 - val_mse: 0.1441\n",
            "Epoch 691/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1088 - mse: 0.1088 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "Epoch 692/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1100 - mse: 0.1100 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 693/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 694/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "Epoch 695/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1112 - mse: 0.1112 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "Epoch 696/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 697/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1116 - mse: 0.1116 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 698/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1089 - mse: 0.1089 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 699/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1111 - mse: 0.1111 - val_loss: 0.1440 - val_mse: 0.1440\n",
            "Epoch 700/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1121 - mse: 0.1121 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 701/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1099 - mse: 0.1099 - val_loss: 0.1443 - val_mse: 0.1443\n",
            "Epoch 702/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 703/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1098 - mse: 0.1098 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 704/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1095 - mse: 0.1095 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 705/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1075 - mse: 0.1075 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 706/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1102 - mse: 0.1102 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 707/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1069 - mse: 0.1069 - val_loss: 0.1447 - val_mse: 0.1447\n",
            "Epoch 708/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1119 - mse: 0.1119 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 709/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.1093 - mse: 0.1093 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 710/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1090 - mse: 0.1090 - val_loss: 0.1441 - val_mse: 0.1441\n",
            "Epoch 711/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1099 - mse: 0.1099 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 712/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1128 - mse: 0.1128 - val_loss: 0.1445 - val_mse: 0.1445\n",
            "Epoch 713/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1098 - mse: 0.1098 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 714/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 715/1000\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.1079 - mse: 0.1079 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 716/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.1095 - mse: 0.1095 - val_loss: 0.1433 - val_mse: 0.1433\n",
            "Epoch 717/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1077 - mse: 0.1077 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 718/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1430 - val_mse: 0.1430\n",
            "Epoch 719/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1082 - mse: 0.1082 - val_loss: 0.1432 - val_mse: 0.1432\n",
            "Epoch 720/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1091 - mse: 0.1091 - val_loss: 0.1432 - val_mse: 0.1432\n",
            "Epoch 721/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1085 - mse: 0.1085 - val_loss: 0.1435 - val_mse: 0.1435\n",
            "Epoch 722/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1108 - mse: 0.1108 - val_loss: 0.1430 - val_mse: 0.1430\n",
            "Epoch 723/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1090 - mse: 0.1090 - val_loss: 0.1433 - val_mse: 0.1433\n",
            "Epoch 724/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1112 - mse: 0.1112 - val_loss: 0.1430 - val_mse: 0.1430\n",
            "Epoch 725/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1432 - val_mse: 0.1432\n",
            "Epoch 726/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1067 - mse: 0.1067 - val_loss: 0.1433 - val_mse: 0.1433\n",
            "Epoch 727/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1113 - mse: 0.1113 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 728/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1089 - mse: 0.1089 - val_loss: 0.1435 - val_mse: 0.1435\n",
            "Epoch 729/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1107 - mse: 0.1107 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 730/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1074 - mse: 0.1074 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 731/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1083 - mse: 0.1083 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 732/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1070 - mse: 0.1070 - val_loss: 0.1433 - val_mse: 0.1433\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 733/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1063 - mse: 0.1063 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 734/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1087 - mse: 0.1087 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 735/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1042 - mse: 0.1042 - val_loss: 0.1434 - val_mse: 0.1434\n",
            "Epoch 736/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1074 - mse: 0.1074 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 737/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1079 - mse: 0.1079 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 738/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 739/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1052 - mse: 0.1052 - val_loss: 0.1433 - val_mse: 0.1433\n",
            "Epoch 740/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1058 - mse: 0.1058 - val_loss: 0.1428 - val_mse: 0.1428\n",
            "Epoch 741/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1073 - mse: 0.1073 - val_loss: 0.1432 - val_mse: 0.1432\n",
            "Epoch 742/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1426 - val_mse: 0.1426\n",
            "Epoch 743/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1086 - mse: 0.1086 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 744/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 745/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1059 - mse: 0.1059 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 746/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1032 - mse: 0.1032 - val_loss: 0.1430 - val_mse: 0.1430\n",
            "Epoch 747/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1101 - mse: 0.1101 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 748/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1059 - mse: 0.1059 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 749/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1055 - mse: 0.1055 - val_loss: 0.1426 - val_mse: 0.1426\n",
            "Epoch 750/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1078 - mse: 0.1078 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 751/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1428 - val_mse: 0.1428\n",
            "Epoch 752/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 753/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1073 - mse: 0.1073 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 754/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1081 - mse: 0.1081 - val_loss: 0.1426 - val_mse: 0.1426\n",
            "Epoch 755/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1050 - mse: 0.1050 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 756/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1060 - mse: 0.1060 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 757/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1077 - mse: 0.1077 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 758/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1080 - mse: 0.1080 - val_loss: 0.1430 - val_mse: 0.1430\n",
            "Epoch 759/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1053 - mse: 0.1053 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 760/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 761/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1058 - mse: 0.1058 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 762/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1070 - mse: 0.1070 - val_loss: 0.1426 - val_mse: 0.1426\n",
            "Epoch 763/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1050 - mse: 0.1050 - val_loss: 0.1429 - val_mse: 0.1429\n",
            "Epoch 764/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1045 - mse: 0.1045 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 765/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1086 - mse: 0.1086 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 766/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1053 - mse: 0.1053 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 767/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1077 - mse: 0.1077 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 768/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1051 - mse: 0.1051 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 769/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1075 - mse: 0.1075 - val_loss: 0.1426 - val_mse: 0.1426\n",
            "Epoch 770/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1041 - mse: 0.1041 - val_loss: 0.1422 - val_mse: 0.1422\n",
            "Epoch 771/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1420 - val_mse: 0.1420\n",
            "Epoch 772/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1040 - mse: 0.1040 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 773/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1078 - mse: 0.1078 - val_loss: 0.1418 - val_mse: 0.1418\n",
            "Epoch 774/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1071 - mse: 0.1071 - val_loss: 0.1423 - val_mse: 0.1423\n",
            "Epoch 775/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1051 - mse: 0.1051 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 776/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1073 - mse: 0.1073 - val_loss: 0.1423 - val_mse: 0.1423\n",
            "Epoch 777/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1428 - val_mse: 0.1428\n",
            "Epoch 778/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1417 - val_mse: 0.1417\n",
            "Epoch 779/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1045 - mse: 0.1045 - val_loss: 0.1423 - val_mse: 0.1423\n",
            "Epoch 780/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1420 - val_mse: 0.1420\n",
            "Epoch 781/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1041 - mse: 0.1041 - val_loss: 0.1422 - val_mse: 0.1422\n",
            "Epoch 782/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1066 - mse: 0.1066 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 783/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1063 - mse: 0.1063 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 784/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 785/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1046 - mse: 0.1046 - val_loss: 0.1423 - val_mse: 0.1423\n",
            "Epoch 786/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1047 - mse: 0.1047 - val_loss: 0.1420 - val_mse: 0.1420\n",
            "Epoch 787/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1079 - mse: 0.1079 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 788/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1056 - mse: 0.1056 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 789/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 790/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 791/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1059 - mse: 0.1059 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 792/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1049 - mse: 0.1049 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 793/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1035 - mse: 0.1035 - val_loss: 0.1418 - val_mse: 0.1418\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 794/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1045 - mse: 0.1045 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 795/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1414 - val_mse: 0.1414\n",
            "Epoch 796/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1025 - mse: 0.1025 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 797/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1071 - mse: 0.1071 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 798/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1052 - mse: 0.1052 - val_loss: 0.1418 - val_mse: 0.1418\n",
            "Epoch 799/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1412 - val_mse: 0.1412\n",
            "Epoch 800/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1052 - mse: 0.1052 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 801/1000\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1069 - mse: 0.1069 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 802/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 803/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1061 - mse: 0.1061 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 804/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1060 - mse: 0.1060 - val_loss: 0.1410 - val_mse: 0.1410\n",
            "Epoch 805/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 806/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1033 - mse: 0.1033 - val_loss: 0.1414 - val_mse: 0.1414\n",
            "Epoch 807/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 808/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1055 - mse: 0.1055 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 809/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1046 - mse: 0.1046 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 810/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 811/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 812/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1046 - mse: 0.1046 - val_loss: 0.1423 - val_mse: 0.1423\n",
            "Epoch 813/1000\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.1032 - mse: 0.1032 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 814/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1033 - mse: 0.1033 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 815/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 816/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1065 - mse: 0.1065 - val_loss: 0.1418 - val_mse: 0.1418\n",
            "Epoch 817/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1047 - mse: 0.1047 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 818/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1059 - mse: 0.1059 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 819/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1042 - mse: 0.1042 - val_loss: 0.1414 - val_mse: 0.1414\n",
            "Epoch 820/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1050 - mse: 0.1050 - val_loss: 0.1417 - val_mse: 0.1417\n",
            "Epoch 821/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 822/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1054 - mse: 0.1054 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 823/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 824/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1021 - mse: 0.1021 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 825/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1052 - mse: 0.1052 - val_loss: 0.1414 - val_mse: 0.1414\n",
            "Epoch 826/1000\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1042 - mse: 0.1042 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 827/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1061 - mse: 0.1061 - val_loss: 0.1417 - val_mse: 0.1417\n",
            "Epoch 828/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1039 - mse: 0.1039 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 829/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 830/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1417 - val_mse: 0.1417\n",
            "Epoch 831/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1034 - mse: 0.1034 - val_loss: 0.1420 - val_mse: 0.1420\n",
            "Epoch 832/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1418 - val_mse: 0.1418\n",
            "Epoch 833/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1414 - val_mse: 0.1414\n",
            "Epoch 834/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1054 - mse: 0.1054 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 835/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1039 - mse: 0.1039 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 836/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1047 - mse: 0.1047 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 837/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1014 - mse: 0.1014 - val_loss: 0.1412 - val_mse: 0.1412\n",
            "Epoch 838/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1033 - mse: 0.1033 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 839/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1020 - mse: 0.1020 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 840/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1000 - mse: 0.1000 - val_loss: 0.1412 - val_mse: 0.1412\n",
            "Epoch 841/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1051 - mse: 0.1051 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 842/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 843/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1053 - mse: 0.1053 - val_loss: 0.1407 - val_mse: 0.1407\n",
            "Epoch 844/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 845/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1033 - mse: 0.1033 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 846/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1016 - mse: 0.1016 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 847/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1037 - mse: 0.1037 - val_loss: 0.1414 - val_mse: 0.1414\n",
            "Epoch 848/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 849/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 850/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 851/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 852/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 853/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1034 - mse: 0.1034 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 854/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1017 - mse: 0.1017 - val_loss: 0.1410 - val_mse: 0.1410\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 855/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1412 - val_mse: 0.1412\n",
            "Epoch 856/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 857/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 858/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1006 - mse: 0.1006 - val_loss: 0.1407 - val_mse: 0.1407\n",
            "Epoch 859/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1407 - val_mse: 0.1407\n",
            "Epoch 860/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1014 - mse: 0.1014 - val_loss: 0.1404 - val_mse: 0.1404\n",
            "Epoch 861/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1017 - mse: 0.1017 - val_loss: 0.1406 - val_mse: 0.1406\n",
            "Epoch 862/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.1406 - val_mse: 0.1406\n",
            "Epoch 863/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1404 - val_mse: 0.1404\n",
            "Epoch 864/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 865/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1034 - mse: 0.1034 - val_loss: 0.1407 - val_mse: 0.1407\n",
            "Epoch 866/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1406 - val_mse: 0.1406\n",
            "Epoch 867/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1406 - val_mse: 0.1406\n",
            "Epoch 868/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 869/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1416 - val_mse: 0.1416\n",
            "Epoch 870/1000\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 871/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1017 - mse: 0.1017 - val_loss: 0.1411 - val_mse: 0.1411\n",
            "Epoch 872/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1034 - mse: 0.1034 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 873/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1040 - mse: 0.1040 - val_loss: 0.1414 - val_mse: 0.1414\n",
            "Epoch 874/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1010 - mse: 0.1010 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 875/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 876/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 877/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1044 - mse: 0.1044 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 878/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1000 - mse: 0.1000 - val_loss: 0.1404 - val_mse: 0.1404\n",
            "Epoch 879/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.1402 - val_mse: 0.1402\n",
            "Epoch 880/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1007 - mse: 0.1007 - val_loss: 0.1404 - val_mse: 0.1404\n",
            "Epoch 881/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1040 - mse: 0.1040 - val_loss: 0.1399 - val_mse: 0.1399\n",
            "Epoch 882/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.1403 - val_mse: 0.1403\n",
            "Epoch 883/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1017 - mse: 0.1017 - val_loss: 0.1401 - val_mse: 0.1401\n",
            "Epoch 884/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.1399 - val_mse: 0.1399\n",
            "Epoch 885/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.1403 - val_mse: 0.1403\n",
            "Epoch 886/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1033 - mse: 0.1033 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 887/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1401 - val_mse: 0.1401\n",
            "Epoch 888/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1406 - val_mse: 0.1406\n",
            "Epoch 889/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 890/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 891/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 892/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 893/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 894/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 895/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1016 - mse: 0.1016 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 896/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 897/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1014 - mse: 0.1014 - val_loss: 0.1400 - val_mse: 0.1400\n",
            "Epoch 898/1000\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 899/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0961 - mse: 0.0961 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 900/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 901/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 902/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 903/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 904/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 905/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.1397 - val_mse: 0.1397\n",
            "Epoch 906/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.1397 - val_mse: 0.1397\n",
            "Epoch 907/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 908/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1010 - mse: 0.1010 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 909/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.1399 - val_mse: 0.1399\n",
            "Epoch 910/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 911/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1397 - val_mse: 0.1397\n",
            "Epoch 912/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 913/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 914/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 915/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.1394 - val_mse: 0.1394\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 916/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 917/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 918/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 919/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 920/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 921/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1384 - val_mse: 0.1384\n",
            "Epoch 922/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 923/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1018 - mse: 0.1018 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 924/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 925/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 926/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 927/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 928/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 929/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 930/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1398 - val_mse: 0.1398\n",
            "Epoch 931/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 932/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 933/1000\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0963 - mse: 0.0963 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 934/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 935/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 936/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 937/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 938/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 939/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 940/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 941/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1011 - mse: 0.1011 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 942/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 943/1000\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 944/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.1397 - val_mse: 0.1397\n",
            "Epoch 945/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 946/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 947/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 948/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0974 - mse: 0.0974 - val_loss: 0.1393 - val_mse: 0.1393\n",
            "Epoch 949/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 950/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 951/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 952/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0974 - mse: 0.0974 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 953/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 954/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 955/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.1387 - val_mse: 0.1387\n",
            "Epoch 956/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1007 - mse: 0.1007 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 957/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 958/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 959/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 960/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0955 - mse: 0.0955 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 961/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 962/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 963/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.1387 - val_mse: 0.1387\n",
            "Epoch 964/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 965/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0974 - mse: 0.0974 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 966/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 967/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0964 - mse: 0.0964 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 968/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0974 - mse: 0.0974 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 969/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0964 - mse: 0.0964 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 970/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 971/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.1387 - val_mse: 0.1387\n",
            "Epoch 972/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 973/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 974/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 975/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 976/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0974 - mse: 0.0974 - val_loss: 0.1385 - val_mse: 0.1385\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 977/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 978/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.1384 - val_mse: 0.1384\n",
            "Epoch 979/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 980/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 981/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 982/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 983/1000\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0969 - mse: 0.0969 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 984/1000\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.1390 - val_mse: 0.1390\n",
            "Epoch 985/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0952 - mse: 0.0952 - val_loss: 0.1384 - val_mse: 0.1384\n",
            "Epoch 986/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.1383 - val_mse: 0.1383\n",
            "Epoch 987/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 988/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1383 - val_mse: 0.1383\n",
            "Epoch 989/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.1386 - val_mse: 0.1386\n",
            "Epoch 990/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.1380 - val_mse: 0.1380\n",
            "Epoch 991/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1378 - val_mse: 0.1378\n",
            "Epoch 992/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.1378 - val_mse: 0.1378\n",
            "Epoch 993/1000\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 994/1000\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.1386 - val_mse: 0.1386\n",
            "Epoch 995/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 996/1000\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0964 - mse: 0.0964 - val_loss: 0.1383 - val_mse: 0.1383\n",
            "Epoch 997/1000\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.1387 - val_mse: 0.1387\n",
            "Epoch 998/1000\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1384 - val_mse: 0.1384\n",
            "Epoch 999/1000\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.1385 - val_mse: 0.1385\n",
            "Epoch 1000/1000\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0963 - mse: 0.0963 - val_loss: 0.1389 - val_mse: 0.1389\n"
          ]
        }
      ],
      "source": [
        "history_drop = model_drop.fit(X_train.values, y_train['cnt'],\n",
        "    validation_data=(X_val.values, y_val['cnt']),\n",
        "    batch_size=batch_size, epochs=n_epochs, verbose=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCQ0ambP7zGK",
        "outputId": "4c64715a-8bb2-45db-f8bf-14acf7f9c83e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(np.arange(len(history_drop.history['loss'])), history_drop.history['loss'], label='training')\n",
        "plt.plot(np.arange(len(history_drop.history['val_loss'])), history_drop.history['val_loss'], label='validation')\n",
        "plt.title('Use dropout for Bike Sharing dataset')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFZkZ9HZ7zGK",
        "outputId": "251ff40d-9aba-4a86-fdb3-f0edf9595c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum loss:  0.13778239488601685 \n",
            "After  990  epochs\n"
          ]
        }
      ],
      "source": [
        "print('Minimum loss: ', min(history_drop.history['val_loss']),\n",
        " '\\nAfter ', np.argmin(history_drop.history['val_loss']), ' epochs')\n",
        "\n",
        "# Minimum loss:  0.126063346863\n",
        "# After  998  epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBaP1vtH7zGK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}